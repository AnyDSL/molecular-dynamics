// Communication offsets (for gather and scatter kernels)
struct CommOffsets {
    // Rank of current process
    me: i32,

    // Host send data
    send_buffer: Buffer,
    send_neighbors: Buffer,
    send_rank_offsets: Buffer,
    send_rank_lengths: Buffer,
    send_offsets: Buffer,
    send_capacity: i32,
    send_noffsets: i32,

    // Host receive data
    recv_buffer: Buffer,
    recv_neighbors: Buffer,
    recv_rank_offsets: Buffer,
    recv_rank_lengths: Buffer,
    recv_offsets: Buffer,
    recv_capacity: i32,
    recv_noffsets: i32,

    // Exchange data
    send_exchg_buffer: Buffer,
    recv_exchg_buffer: Buffer,
    exchg_rank_send_lengths: Buffer,
    exchg_rank_recv_lengths: Buffer,
    exchg_rank_send_offsets: Buffer,

    // Accelerator send data
    send_buffer_accelerator: Buffer,
    send_offsets_accelerator: Buffer,

    // Accelerator receive data
    recv_buffer_accelerator: Buffer,
    recv_offsets_accelerator: Buffer,

    // Maximun and current number of neighbors
    max_neighs: i32,
    neighs: i32
};

// Number of nodes in each dimension
static mut gx: i32;
static mut gy: i32;
static mut gz: i32;

// Cartesian info
static mut myloc: [i32 * 3];
static mut xprev: i32;
static mut xnext: i32;
static mut yprev: i32;
static mut ynext: i32;
static mut zprev: i32;
static mut znext: i32;

// C functions to call walberla procedures
extern "C" {
    fn use_walberla() -> bool;
    fn get_number_of_neighbor_ranks() -> u32;
    fn get_neighborhood_rank(i32) -> i32;
    fn in_rank_border(i32, f64, f64, f64, f64) -> bool;
    fn in_rank_subdomain(i32, f64, f64, f64) -> bool;
}

// Maximum number of neighbor ranks
fn @get_initial_maximum_neighbor_ranks() -> i32 { select(use_walberla(), 30, 6) }

// Types for condition and communication functions
type CondFunc = fn(Vector3D, &Grid, PBCFlags) -> bool;
type CommFunc = fn(i32, i32, CondFunc, CondFunc) -> ();

// Communication pattern using walberla or 6-stencil neighbors
fn communication_ranks(body: CommFunc) -> () {
    if use_walberla() {
        let nranks = get_number_of_neighbor_ranks();

        range(0, nranks as i32, |i| {
            let rank = get_neighborhood_rank(i);

            body(rank, rank,
                |p, g, _| { in_rank_border(rank, p.x, p.y, p.z, g.spacing) },
                |p, _, _| { in_rank_subdomain(rank, p.x, p.y, p.z) });
        });
    } else {
        body(xnext, xprev,
            |p, g, pbc| { p.x > g.aabb.xmax - g.spacing * 2.0 },
            |p, g, pbc| { (pbc.x < 0 as i8) || (p.x > g.aabb.xmax - g.spacing) });

        body(xprev, xnext,
            |p, g, pbc| { p.x < g.aabb.xmin + g.spacing * 2.0 },
            |p, g, pbc| { (pbc.x > 0 as i8) || (p.x < g.aabb.xmin + g.spacing) });

        body(ynext, yprev,
            |p, g, pbc| { p.y > g.aabb.ymax - g.spacing * 2.0 },
            |p, g, pbc| { (pbc.y < 0 as i8) || (p.y > g.aabb.ymax - g.spacing) });

        body(yprev, ynext,
            |p, g, pbc| { p.y < g.aabb.ymin + g.spacing * 2.0 },
            |p, g, pbc| { (pbc.y > 0 as i8) || (p.y < g.aabb.ymin + g.spacing) });

        body(znext, zprev,
            |p, g, pbc| { p.z > g.aabb.zmax - g.spacing * 2.0 },
            |p, g, pbc| { (pbc.z < 0 as i8) || (p.z > g.aabb.zmax - g.spacing) });

        body(zprev, znext,
            |p, g, pbc| { p.z < g.aabb.zmin + g.spacing * 2.0 },
            |p, g, pbc| { (pbc.z > 0 as i8) || (p.z < g.aabb.zmin + g.spacing) });
    }
}

// Get world size
fn get_world_size() -> i32 {
    let mpih = mpi();
    let mut world_size: i32;

    mpih.comm_size(mpih.comms.world, &mut world_size);
    world_size
}

// Get process rank
fn get_process_rank() -> i32 {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);
    rank
}

// MPI barrier
fn barrier() -> () {
    let mpih = mpi();
    let mut request: MPI_Request;

    mpih.barrier(mpih.comms.world, &mut request);
}

// Print string with rank
fn print_string_with_rank(string: &[u8]) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(string);
    print_string("\n");
    print_flush();
}

// Print i32 value with rank
fn print_i32_with_rank(field: &[u8], value: i32) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_i32(value);
    print_string("\n");
    print_flush();
}

// Print real value with rank
fn print_real_with_rank(field: &[u8], value: real_t) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_f64(value);
    print_string("\n");
    print_flush();
}

// Print real buffer with rank
fn print_real_buffer_with_rank(field: &[u8], buffer: Buffer, offset: i32, length: i32) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");

    range(0, length, |i| {
        print_f64(get_real(i + offset, buffer));
        print_string(", ");
    });

    print_string("\n");
}

// Print i32 buffer with rank
fn print_i32_buffer_with_rank(field: &[u8], buffer: Buffer, offset: i32, length: i32) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");

    range(0, length, |i| {
        print_i32(get_i32(i + offset, buffer));
        print_string(", ");
    });

    print_string("\n");
}

// Print [i32 * 3] value with rank
fn print_i32_vector_with_rank(field: &[u8], value: [i32 * 3]) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_i32(value(0));
    print_string(", ");
    print_i32(value(1));
    print_string(", ");
    print_i32(value(2));
    print_string("\n");
    print_flush();
}

// Print Vector3D value with rank
fn print_real_vector_with_rank(field: &[u8], value: Vector3D) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_f64(value.x);
    print_string(", ");
    print_f64(value.y);
    print_string(", ");
    print_f64(value.z);
    print_string("\n");
    print_flush();
}

// Print AABB with rank
fn print_aabb_with_rank(field: &[u8], value: AABB) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": xrange = <");
    print_f64(value.xmin);
    print_string(", ");
    print_f64(value.xmax);
    print_string("> yrange = <");
    print_f64(value.ymin);
    print_string(", ");
    print_f64(value.ymax);
    print_string("> zrange = <");
    print_f64(value.zmin);
    print_string(", ");
    print_f64(value.zmax);
    print_string(">\n");
    print_flush();
}

// Initialize MPI
fn mpi_initialize() -> () {
    mpi().init();
}

// Finalize MPI and free data structures for communication
fn mpi_finalize() -> () {
    release_comm_offsets(comm_offsets_);
    mpi().finalize();
}

// Release communication buffers
fn release_comm_offsets(comm_offsets: CommOffsets) -> () {
    if !use_unified_memory() {
        release(comm_offsets.send_buffer);
        release(comm_offsets.recv_buffer);
        release(comm_offsets.send_offsets);
        release(comm_offsets.recv_offsets);
    }

    if comm_offsets.send_capacity > 0 {
        release(comm_offsets.send_neighbors);
        release(comm_offsets.send_rank_offsets);
        release(comm_offsets.send_rank_lengths);
        release(comm_offsets.send_buffer_accelerator);
        release(comm_offsets.send_offsets_accelerator);
        release(comm_offsets.send_exchg_buffer);
        release(comm_offsets.exchg_rank_send_lengths);
    }

    if comm_offsets.recv_capacity > 0 {
        release(comm_offsets.recv_neighbors);
        release(comm_offsets.recv_rank_offsets);
        release(comm_offsets.recv_rank_lengths);
        release(comm_offsets.recv_buffer_accelerator);
        release(comm_offsets.recv_offsets_accelerator);
        release(comm_offsets.recv_exchg_buffer);
        release(comm_offsets.exchg_rank_recv_lengths);
    }

    release(comm_offsets.exchg_rank_send_offsets);
}

fn resize_max_neighbors_capacity(comm_offsets: &mut CommOffsets, max_neighs: i32) -> () {
    print_i32_with_rank("resize_max_neighbors_capacity()", max_neighs);

    reallocate_buffer(&mut comm_offsets.send_neighbors, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.send_rank_offsets, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.send_rank_lengths, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.recv_neighbors, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.recv_rank_offsets, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.recv_rank_lengths, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.exchg_rank_send_lengths, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.exchg_rank_recv_lengths, max_neighs, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.exchg_rank_send_offsets, max_neighs, sizeof[i32](), true, cpu_allocate);

    comm_offsets.max_neighs = max_neighs;
}

fn resize_send_capacity(comm_offsets: &mut CommOffsets, send_capacity: i32) -> () {
    print_i32_with_rank("resize_send_capacity()", send_capacity);

    reallocate_buffer(&mut comm_offsets.send_buffer, send_capacity * 7, sizeof[real_t](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.send_offsets, send_capacity, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.send_exchg_buffer, send_capacity * 7, sizeof[real_t](), true, cpu_allocate);

    release(comm_offsets.send_buffer_accelerator);
    release(comm_offsets.send_offsets_accelerator);

    comm_offsets.send_buffer_accelerator = accelerator_allocate(send_capacity * 3 * sizeof[real_t]());
    comm_offsets.send_offsets_accelerator = accelerator_allocate(send_capacity * sizeof[i32]());
    comm_offsets.send_capacity = send_capacity;

    assign_accelerator_buffer(&mut comm_offsets.send_buffer, comm_offsets.send_buffer_accelerator);
    assign_accelerator_buffer(&mut comm_offsets.send_offsets, comm_offsets.send_offsets_accelerator);
}

fn resize_recv_capacity(comm_offsets: &mut CommOffsets, recv_capacity: i32) -> () {
    print_i32_with_rank("resize_recv_capacity()", recv_capacity);

    reallocate_buffer(&mut comm_offsets.recv_buffer, recv_capacity * 7, sizeof[real_t](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.recv_offsets, recv_capacity, sizeof[i32](), true, cpu_allocate);
    reallocate_buffer(&mut comm_offsets.recv_exchg_buffer, recv_capacity * 7, sizeof[real_t](), true, cpu_allocate);

    release(comm_offsets.recv_buffer_accelerator);
    release(comm_offsets.recv_offsets_accelerator);

    comm_offsets.recv_buffer_accelerator = accelerator_allocate(recv_capacity * 3 * sizeof[real_t]());
    comm_offsets.recv_offsets_accelerator = accelerator_allocate(recv_capacity * sizeof[i32]());
    comm_offsets.recv_capacity = recv_capacity;

    assign_accelerator_buffer(&mut comm_offsets.recv_buffer, comm_offsets.recv_buffer_accelerator);
    assign_accelerator_buffer(&mut comm_offsets.recv_offsets, comm_offsets.recv_offsets_accelerator);
}

// Get configuration for nodes according to world size and number of
// cells in each dimension
fn get_node_config(xlength: real_t, ylength: real_t, zlength: real_t, destx: &mut i32, desty: &mut i32, destz: &mut i32) -> () {
    let mpih = mpi();
    let areax = xlength * ylength;
    let areay = xlength * zlength;
    let areaz = ylength * zlength;
    let mut bestsurf = 2.0 * (areax + areay + areaz) as f64;
    let mut world_size: i32;

    *destx = 1;
    *desty = 1;
    *destz = 1;

    mpih.comm_size(mpih.comms.world, &mut world_size);

    for i in range(1, world_size) {
        if world_size % i == 0 {
            let rem_yz = world_size / i;

            for j in range(1, rem_yz) {
                if rem_yz % j == 0 {
                    let k = rem_yz / j;
                    let surf = areax / i as f64 / j as f64 + areay / i as f64 / k as f64 + areaz / j as f64 / k as f64;

                    if surf < bestsurf {
                        *destx = i;
                        *desty = j;
                        *destz = k;
                        bestsurf = surf;
                    }
                }
            }
        }
    }
}

// Get bounding box for current node
fn @get_node_bounding_box(aabb: AABB) -> AABB {
    let mpih = mpi();

    let mut xmin: real_t;
    let mut xmax: real_t;
    let mut ymin: real_t;
    let mut ymax: real_t;
    let mut zmin: real_t;
    let mut zmax: real_t;

    // Number of cells in each dimension
    let xtotallength = aabb.xmax - aabb.xmin;
    let ytotallength = aabb.ymax - aabb.ymin;
    let ztotallength = aabb.zmax - aabb.zmin;

    // Get configuration of nodes
    get_node_config(xtotallength, ytotallength, ztotallength, &mut gx, &mut gy, &mut gz);

    // Dimensions length for each rank
    let xlength = xtotallength / (gx as real_t);
    let ylength = ytotallength / (gy as real_t);
    let zlength = ztotallength / (gz as real_t);

    let mut locx: i32;
    let mut locy: i32;
    let mut locz: i32;

    // 3D cartesian position of current rank
    mpih.cart(gx, gy, gz, &mut locx, &mut locy, &mut locz, &mut xprev, &mut xnext, &mut yprev, &mut ynext, &mut zprev, &mut znext);

    // Location for my rank
    myloc(0) = locx;
    myloc(1) = locy;
    myloc(2) = locz;

    // Calculate boundaries using lengths in each dimension
    xmin = aabb.xmin + xlength * (myloc(0) as real_t);
    xmax = xmin + xlength;
    ymin = aabb.ymin + ylength * (myloc(1) as real_t);
    ymax = ymin + ylength;
    zmin = aabb.zmin + zlength * (myloc(2) as real_t);
    zmax = zmin + zlength;

    AABB {
        xmin: xmin,
        xmax: xmax,
        ymin: ymin,
        ymax: ymax,
        zmin: zmin,
        zmax: zmax
    }
}

// Initialize grid communication
fn initialize_comm_offsets(grid: &Grid, comm_offsets: &mut CommOffsets) -> () {
    let mpih = mpi();
    let max_neighs = get_initial_maximum_neighbor_ranks();
    let max_faces_dim = math.max(math.max(grid.nx * grid.ny, grid.nx * grid.nz), grid.ny * grid.nz);
    let send_capacity = max_neighs * max_faces_dim * 20;
    let recv_capacity = max_neighs * max_faces_dim * 20;

    let null_buf = Buffer {
        device: 0,
        data: 0 as &[i8],
        size: 0 as i64
    };

    let mut world_rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut world_rank);

    if send_capacity > 0 && recv_capacity > 0 {
        let max_neighs = get_initial_maximum_neighbor_ranks();

        *comm_offsets = CommOffsets {
            // Rank of current process
            me: world_rank,

            // Host send data
            send_buffer: cpu_allocate(send_capacity * 7 * sizeof[real_t]()),
            send_neighbors: cpu_allocate(max_neighs * sizeof[i32]()),
            send_rank_offsets: cpu_allocate(max_neighs * sizeof[i32]()),
            send_rank_lengths: cpu_allocate(max_neighs * sizeof[i32]()),
            send_offsets: cpu_allocate(send_capacity * sizeof[i32]()),
            send_capacity: send_capacity,
            send_noffsets: 0,

            // Host receive data
            recv_buffer: cpu_allocate(recv_capacity * 7 * sizeof[real_t]()),
            recv_neighbors: cpu_allocate(max_neighs * sizeof[i32]()),
            recv_rank_offsets: cpu_allocate(max_neighs * sizeof[i32]()),
            recv_rank_lengths: cpu_allocate(max_neighs * sizeof[i32]()),
            recv_offsets: cpu_allocate(recv_capacity * sizeof[i32]()),
            recv_capacity: recv_capacity,
            recv_noffsets: 0,

            // Exchange data
            send_exchg_buffer: cpu_allocate(send_capacity * 7 * sizeof[real_t]()),
            recv_exchg_buffer: cpu_allocate(recv_capacity * 7 * sizeof[real_t]()),
            exchg_rank_send_lengths: cpu_allocate(max_neighs * sizeof[i32]()),
            exchg_rank_recv_lengths: cpu_allocate(max_neighs * sizeof[i32]()),
            exchg_rank_send_offsets: cpu_allocate(max_neighs * sizeof[i32]()),

            // Accelerator send data
            send_buffer_accelerator: accelerator_allocate(send_capacity * 3 * sizeof[real_t]()),
            send_offsets_accelerator: accelerator_allocate(send_capacity * sizeof[i32]()),

            // Accelerator receive data
            recv_buffer_accelerator: accelerator_allocate(recv_capacity * 3 * sizeof[real_t]()),
            recv_offsets_accelerator: accelerator_allocate(recv_capacity * sizeof[i32]()),

            // Maximum and current number of neighbor ranks
            max_neighs: max_neighs,
            neighs: 0
        };

        assign_accelerator_buffer(&mut comm_offsets.send_buffer, comm_offsets.send_buffer_accelerator);
        assign_accelerator_buffer(&mut comm_offsets.send_offsets, comm_offsets.send_offsets_accelerator);
        assign_accelerator_buffer(&mut comm_offsets.recv_buffer, comm_offsets.recv_buffer_accelerator);
        assign_accelerator_buffer(&mut comm_offsets.recv_offsets, comm_offsets.recv_offsets_accelerator);
    }
}

// Synchronize ghost layer cells with neighbor ranks
fn synchronize_ghost_layer(grid: Grid, comm_offsets: CommOffsets) -> () {
    let mpih = mpi();
    let mut request: MPI_Request;
    let mut status: MPIStatus;

    let send_neighbors = get_array_of_i32(comm_offsets.send_neighbors);
    let recv_neighbors = get_array_of_i32(comm_offsets.recv_neighbors);
    let send_rank_offsets = get_array_of_i32(comm_offsets.send_rank_offsets);
    let recv_rank_offsets = get_array_of_i32(comm_offsets.recv_rank_offsets);
    let send_rank_lengths = get_array_of_i32(comm_offsets.send_rank_lengths);
    let recv_rank_lengths = get_array_of_i32(comm_offsets.recv_rank_lengths);

    gather_data(grid, comm_offsets);

    range(0, comm_offsets.neighs, |neigh| {
        let send_offset = send_rank_offsets(neigh) * 3 * sizeof[real_t]();
        let recv_offset = recv_rank_offsets(neigh) * 3 * sizeof[real_t]();

        if send_neighbors(neigh) != comm_offsets.me && recv_neighbors(neigh) != comm_offsets.me {
            mpih.irecv(
                &comm_offsets.recv_buffer.data(recv_offset) as MPI_MutBuf, recv_rank_lengths(neigh) * 3,
                mpih.double_t, recv_neighbors(neigh), 0, mpih.comms.world, &mut request);

            mpih.send(
                &comm_offsets.send_buffer.data(send_offset) as MPI_MutBuf, send_rank_lengths(neigh) * 3,
                mpih.double_t, send_neighbors(neigh), 0, mpih.comms.world);

            mpih.wait(&request, &mut status);
        } else {
            copy_offset(comm_offsets.send_buffer, send_offset, comm_offsets.recv_buffer, recv_offset, recv_rank_lengths(neigh) * 3);
        }
    });

    scatter_data(grid, comm_offsets);
}

fn copy_particle_data_to_buffer(mass: real_t, pos: Vector3D, vel: Vector3D, buffer: Buffer, index: i32) -> i32 {
    let mut buffer_index = index;

    bitcast[&mut[real_t]](buffer.data)(buffer_index++) = mass;
    bitcast[&mut[real_t]](buffer.data)(buffer_index++) = pos.x;
    bitcast[&mut[real_t]](buffer.data)(buffer_index++) = pos.y;
    bitcast[&mut[real_t]](buffer.data)(buffer_index++) = pos.z;
    bitcast[&mut[real_t]](buffer.data)(buffer_index++) = vel.x;
    bitcast[&mut[real_t]](buffer.data)(buffer_index++) = vel.y;
    bitcast[&mut[real_t]](buffer.data)(buffer_index++) = vel.z;

    buffer_index
}

fn copy_buffer_data_to_grid(buffer: Buffer, buffer_index: i32, grid: &Grid, offset: i32) -> i32 {
    let mut index = buffer_index;

    copy_offset(buffer, index * sizeof[real_t](), grid.masses_cpu, offset * sizeof[real_t](), sizeof[real_t]());
    index++;

    copy_buffer_to_3d_arrays(buffer, index * sizeof[real_t](), grid.positions_cpu, offset * sizeof[real_t](), sizeof[real_t]());
    index += 3;

    copy_buffer_to_3d_arrays(buffer, index * sizeof[real_t](), grid.velocities_cpu, offset * sizeof[real_t](), sizeof[real_t]());
    index + 3
}

// Exchange ghost layer particles with neighbor ranks (here the number of
// particles is also updated)
fn exchange_particles(grid: &mut Grid, comm_offsets: &mut CommOffsets) -> () {
    let mpih = mpi();
    let mut request: MPI_Request;
    let mut status: MPIStatus;
    let mut iexchg = 0;
    let mut neigh = 0;
    let mut exchg_index = 0;

    grid.nghost = 0;

    communication_ranks(|_, _, _, _| {
        if neigh >= comm_offsets.max_neighs {
            resize_max_neighbors_capacity(comm_offsets, neigh + 8);
        }

        let exchg_rank_send_offsets = get_array_of_i32(comm_offsets.exchg_rank_send_offsets);
        exchg_rank_send_offsets(neigh) = -1;
        neigh++;
    });

    let exchg_rank_send_offsets = get_array_of_i32(comm_offsets.exchg_rank_send_offsets);
    let exchg_rank_send_lengths = get_array_of_i32(comm_offsets.exchg_rank_send_lengths);
    let exchg_rank_recv_lengths = get_array_of_i32(comm_offsets.exchg_rank_recv_lengths);

    neigh = 0;

    // Pack particles in each direction to exchange
    communication_ranks(|exchg_rank, _, _, check_exchange| {
        exchg_rank_send_offsets(neigh) = iexchg;

        // Makes no sense to exchange data with myself, right?
        if exchg_rank != comm_offsets.me {
            let mut particle_index = 0;

            while particle_index < grid.nparticles {
                let pos = get_vector_from_3d_arrays(particle_index, grid.positions_cpu);
                let pbc = get_pbc_flags(particle_index, *grid);

                if check_exchange(pos, grid, pbc) {
                    let mass = get_real(particle_index, grid.masses_cpu);
                    let velocity = get_vector_from_3d_arrays(particle_index, grid.velocities_cpu);

                    if iexchg >= comm_offsets.send_capacity {
                        resize_send_capacity(comm_offsets, iexchg * 2);
                    }

                    exchg_index = copy_particle_data_to_buffer(mass, pos, velocity, comm_offsets.send_exchg_buffer, exchg_index);
                    delete_particle(particle_index, grid);
                    particle_index--;
                    iexchg++;
                }

                particle_index++;
            }
        }

        exchg_rank_send_lengths(neigh) = iexchg - exchg_rank_send_offsets(neigh);
        neigh++;
    });

    iexchg = 0;
    neigh = 0;

    // Exchange particles with other ranks
    communication_ranks(|send_rank, recv_rank, _, _| {
        if send_rank != comm_offsets.me {
            let exchg_send_offset = exchg_rank_send_offsets(neigh) * 7 * sizeof[real_t]();
            let exchg_recv_offset = iexchg * 7 * sizeof[real_t]();

            // Exchange sizes
            mpih.send(&mut exchg_rank_send_lengths(neigh) as MPI_MutBuf, 1, mpih.int_t, send_rank, 0, mpih.comms.world);
            mpih.recv(&mut exchg_rank_recv_lengths(neigh) as MPI_MutBuf, 1, mpih.int_t, recv_rank, 0, mpih.comms.world, &mut status);

            // Readjust receive capacity if it is not enough
            if iexchg + exchg_rank_recv_lengths(neigh) >= comm_offsets.recv_capacity {
                resize_recv_capacity(comm_offsets, (iexchg + exchg_rank_recv_lengths(neigh)) * 2);
            }

            // Exchange data
            mpih.irecv(
                &comm_offsets.recv_exchg_buffer.data(exchg_recv_offset) as MPI_MutBuf, exchg_rank_recv_lengths(neigh) * 7,
                mpih.double_t, recv_rank, 0, mpih.comms.world, &mut request);

            mpih.send(
                &comm_offsets.send_exchg_buffer.data(exchg_send_offset) as MPI_MutBuf, exchg_rank_send_lengths(neigh) * 7,
                mpih.double_t, send_rank, 0, mpih.comms.world);

            mpih.wait(&request, &mut status);

            // Adjust exchange offset data
            iexchg += exchg_rank_recv_lengths(neigh);
        }

        neigh++;
    });

    // Unpack received particles
    let exchg_start = grid.nparticles;
    if iexchg > 0 { add_local_slots(iexchg, grid); }

    exchg_index = 0;
    range(0, iexchg, |i| {
        exchg_index = copy_buffer_data_to_grid(comm_offsets.recv_exchg_buffer, exchg_index, grid, exchg_start + i);
    });
}

// Define particle borders to synchronize during next iterations
fn borders(grid: &mut Grid, comm_offsets: &mut CommOffsets) -> () {
    let mpih = mpi();
    let mut request: MPI_Request;
    let mut status: MPIStatus;
    let mut isend = 0;
    let mut irecv = 0;
    let mut neigh = 0;
    let mut buffer_index = 0;

    grid.nghost = 0;

    communication_ranks(|_, _, _, _| {
        if neigh >= comm_offsets.max_neighs {
            resize_max_neighbors_capacity(comm_offsets, neigh + 8);
        }

        let send_rank_offsets = get_array_of_i32(comm_offsets.send_rank_offsets);
        let recv_rank_offsets = get_array_of_i32(comm_offsets.recv_rank_offsets);

        send_rank_offsets(neigh) = -1;
        recv_rank_offsets(neigh) = -1;

        neigh++;
    });

    let send_rank_offsets = get_array_of_i32(comm_offsets.send_rank_offsets);
    let send_rank_lengths = get_array_of_i32(comm_offsets.send_rank_lengths);

    neigh = 0;

    // Pack particles in each direction to send during next iterations
    communication_ranks(|_, _, check_border, _| {
        send_rank_offsets(neigh) = isend;

        range(0, grid.nparticles, |particle_index| {
            let pos = get_vector_from_3d_arrays(particle_index, grid.positions_cpu);
            let pbc = PBCFlags { x: 0 as i8, y: 0 as i8, z: 0 as i8 };

            if check_border(pos, grid, pbc) {
                let mass = get_real(particle_index, grid.masses_cpu);
                let velocity = get_vector_from_3d_arrays(particle_index, grid.velocities_cpu);

                if isend >= comm_offsets.send_capacity {
                    resize_send_capacity(comm_offsets, isend * 2);
                }

                let send_offsets = get_array_of_i32(comm_offsets.send_offsets);
                buffer_index = copy_particle_data_to_buffer(mass, pos, velocity, comm_offsets.send_buffer, buffer_index);
                send_offsets(isend++) = particle_index;
            }
        });

        send_rank_lengths(neigh) = isend - send_rank_offsets(neigh);
        neigh++;
    });

    let recv_rank_offsets = get_array_of_i32(comm_offsets.recv_rank_offsets);
    let recv_rank_lengths = get_array_of_i32(comm_offsets.recv_rank_lengths);
    let send_neighbors = get_array_of_i32(comm_offsets.send_neighbors);
    let recv_neighbors = get_array_of_i32(comm_offsets.recv_neighbors);
    let mut recv_offsets = get_array_of_i32(comm_offsets.recv_offsets);

    neigh = 0;

    // Synchronize borders with other ranks
    communication_ranks(|send_rank, recv_rank, _, _| {
        // Rank receive offset
        recv_rank_offsets(neigh) = irecv;

        // Offsets to send and receive
        let send_offset = send_rank_offsets(neigh) * 7 * sizeof[real_t]();
        let recv_offset = recv_rank_offsets(neigh) * 7 * sizeof[real_t]();

        // Send sizes
        if send_rank != comm_offsets.me {
            mpih.send(&mut send_rank_lengths(neigh) as MPI_MutBuf, 1, mpih.int_t, send_rank, 0, mpih.comms.world);
            mpih.recv(&mut recv_rank_lengths(neigh) as MPI_MutBuf, 1, mpih.int_t, recv_rank, 0, mpih.comms.world, &mut status);
        } else {
            recv_rank_lengths(neigh) = send_rank_lengths(neigh);
        }

        // Readjust receive capacity if it is not enough
        if irecv + recv_rank_lengths(neigh) >= comm_offsets.recv_capacity {
            resize_recv_capacity(comm_offsets, (irecv + recv_rank_lengths(neigh)) * 2);
            recv_offsets = get_array_of_i32(comm_offsets.recv_offsets);
        }

        // Send and receive data
        if send_rank != comm_offsets.me {
            mpih.irecv(
                &comm_offsets.recv_buffer.data(recv_offset) as MPI_MutBuf, recv_rank_lengths(neigh) * 7,
                mpih.double_t, recv_rank, 0, mpih.comms.world, &mut request);

            mpih.send(
                &comm_offsets.send_buffer.data(send_offset) as MPI_MutBuf, send_rank_lengths(neigh) * 7,
                mpih.double_t, send_rank, 0, mpih.comms.world);

            mpih.wait(&request, &mut status);
        } else {
            copy_offset(comm_offsets.send_buffer, send_offset, comm_offsets.recv_buffer, recv_offset, recv_rank_lengths(neigh) * 7);
        }

        // Update ranks to send and receive during synchronization
        send_neighbors(neigh) = send_rank;
        recv_neighbors(neigh) = recv_rank;

        // Adjust receive offset data
        range(irecv, irecv + recv_rank_lengths(neigh), |i| { recv_offsets(i) = grid.nparticles + i; });
        irecv += recv_rank_lengths(neigh);
        neigh++;
    });

    if irecv > 0 { add_ghost_slots(irecv, grid); }

    buffer_index = 0;
    range(0, irecv, |i| {
        buffer_index = copy_buffer_data_to_grid(comm_offsets.recv_buffer, buffer_index, grid, recv_offsets(i));
    });

    comm_offsets.send_noffsets = isend;
    comm_offsets.recv_noffsets = irecv;
    transfer_between_devices(comm_offsets.send_offsets, comm_offsets.send_offsets_accelerator);
    transfer_between_devices(comm_offsets.recv_offsets, comm_offsets.recv_offsets_accelerator);
    comm_offsets.neighs = neigh;
}

fn reduce_time(local_time: f64, global_time: &mut f64) -> () {
    let mpih = mpi();
    let mut local = local_time;
    mpih.allreduce(&mut local as MPI_MutBuf, global_time as MPI_MutBuf, 1, mpih.double_t, mpih.ops.max, mpih.comms.world);
}

fn reduce_i32_sum(local_value: i32, global_value: &mut i32) -> () {
    let mpih = mpi();
    let mut local = local_value;
    mpih.allreduce(&mut local as MPI_MutBuf, global_value as MPI_MutBuf, 1, mpih.int_t, mpih.ops.sum, mpih.comms.world);
}

fn reduce_i64_sum(local_value: i64, global_value: &mut i64) -> () {
    let mpih = mpi();
    let mut local = local_value;
    mpih.allreduce(&mut local as MPI_MutBuf, global_value as MPI_MutBuf, 1, mpih.int64_t, mpih.ops.sum, mpih.comms.world);
}
