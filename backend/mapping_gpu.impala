fn @is_x86() -> bool { false }
fn @is_sse() -> bool { false }
fn @is_avx() -> bool { false }
fn @is_avx2() -> bool { false }

type dev_i32_ptr = &mut[1]i32;

struct ReductionBuffers {
    buffer_host: Buffer,
    buffer_gpu: Buffer,
    capacity: i32
}

static mut reduction_buffers_: ReductionBuffers;

fn @device() -> Device {
    Device {
        cpu_target: @|| { false },
        init: |grid| {
            let acc = accelerator(device_id);
            let grid_size = (32, 1, 1);
            let block_size = (32, 1, 1);
            reduction_buffers_.capacity = 0;
            acc.exec(grid_size, block_size, |work_item| {});
        },
        shutdown: || {
            if reduction_buffers_.capacity > 0 {
                release(reduction_buffers_.buffer_host);
                release(reduction_buffers_.buffer_gpu);
            }
        },
        alloc: |size| { accelerator(device_id).alloc(size) },
        alloc_mirror: |buf, size| { alloc_cpu(size) },
        transfer: |from, to| { copy(from, to); },
        sqrt: @|a| { nvvm_intrinsics.sqrt(a) },
        add_iterator: |iterator| {},
        atomic_add_i32: @|ptr, value| { nvvm_atomic_add_global(ptr as &mut[1]i32, value) }
    }
}

fn @(?ghost) particles(@ghost: bool, grid: Grid, body: fn(i32, &mut[i32], i32, i32) -> ()) -> () {
    let neighbors_sizes = get_array_i32_ref(array_dev, grid.neighbors_sizes);
    let neighborlists = get_array_i32_ref(array_dev, grid.neighborlists);
    let neighborlist_capacity = grid.neighborlist_capacity;
    let n = select(ghost, grid.nparticles + grid.nghost, grid.nparticles);
    let acc = accelerator(device_id);
    let block_size = (64, 1, 1);
    let grid_size  = (round_up(n, block_size(0)), 1, 1);

    if n > 0 {
        acc.exec(grid_size, block_size, |work_item| {
            let particle_index = work_item.bidx() * work_item.bdimx() + work_item.tidx();
            if particle_index < n {
                let nb_list_offset = neighborlist_capacity * particle_index;
                let nb_list_size = neighbors_sizes(particle_index);
                @@body(particle_index, neighborlists, nb_list_size, nb_list_offset);
            }
        });

        acc.sync();
    }
}

fn @particles_vec(@ghost: bool, grid: Grid, f: fn(i32, &mut[i32], i32, i32) -> ()) -> () { particles(ghost, grid, f); }

fn cells(grid: Grid, body: fn(i32) -> ()) -> () {
    let ncells = grid.ncells;
    let acc = accelerator(device_id);
    let block_size = (64, 1, 1);
    let grid_size  = (round_up(ncells, block_size(0)), 1, 1);

    if ncells > 0 {
        acc.exec(grid_size, block_size, |work_item| {
            let cell_index = work_item.bidx() * work_item.bdimx() + work_item.tidx();
            if cell_index < ncells {
                @@body(cell_index);
            }
        });

        acc.sync();
    }
}

fn copy_list_iterate(comm_offsets: CommOffsets, ncopy: i32, body: fn(i32, &[i32], &[i32]) -> ()) -> () {
    let acc = accelerator(device_id);
    let block_size = (64, 1, 1);
    let grid_size = (round_up(ncopy, block_size(0)), 1, 1);

    if ncopy > 0 {
        acc.exec(grid_size, block_size, |work_item| {
            let index = work_item.bidx() * work_item.bdimx() + work_item.tidx();
            if index < ncopy {
                @@body(index, get_array_i32_ref(array_dev, comm_offsets.copy_list), get_array_i32_ref(array_dev, comm_offsets.send_offsets));
            }
        });

        acc.sync();
    }
}

fn comm_buffer_iterate(comm_offsets: CommOffsets, noffsets: i32, body: fn(i32, &mut[real_t], &[real_t]) -> ()) -> () {
    let acc = accelerator(device_id);
    let block_size = (64, 1, 1);
    let grid_size = (round_up(noffsets, block_size(0)), 1, 1);

    if noffsets > 0 {
        acc.exec(grid_size, block_size, |work_item| {
            let index = work_item.bidx() * work_item.bdimx() + work_item.tidx();
            if index < noffsets {
                @@body(index, get_array_real_ref(array_dev, comm_offsets.send_buffer), get_array_real_ref(array_dev, comm_offsets.recv_buffer));
            }
        });

        acc.sync();
    }
}

fn @reserve_reduction_buffers(nelements: i32, elem_size: i32) -> () {
    let dev = device();

    if reduction_buffers_.capacity < nelements * elem_size {
        let new_capacity = (nelements + 20) * elem_size;
        release(reduction_buffers_.buffer_host);
        release(reduction_buffers_.buffer_gpu);
        reduction_buffers_.buffer_host = alloc_cpu(new_capacity);
        reduction_buffers_.buffer_gpu = dev.alloc(new_capacity);
        reduction_buffers_.capacity = new_capacity;
    }
}

fn @reduce_i32(n: i32, b: i32, reduce: fn(i32, i32) -> i32, body: fn(i32) -> i32) -> i32 {
    let dev = device();
    let acc = accelerator(device_id);
    let block_size = (64, 1, 1);
    let nblocks = round_up(n, block_size(0));
    let grid_size = (nblocks, 1, 1);
    let mut red = b;

    if n > 0 {
        reserve_reduction_buffers(nblocks, sizeof[i32]());
        let blocks_red_host = get_array_of_i32(reduction_buffers_.buffer_host);
        let blocks_red_gpu = get_array_of_i32_accelerator(reduction_buffers_.buffer_gpu);

        acc.exec(grid_size, block_size, |work_item| {
            let sh_red_data = reserve_shared[i32](block_size(0));
            let tid = work_item.tidx();
            let index = work_item.bidx() * work_item.bdimx() + work_item.tidx();

            if index < n {
                sh_red_data(tid) = body(index);
            } else {
                sh_red_data(tid) = b;
            }

            acc.barrier();

            let mut s = work_item.bdimx() >> 1;
            while s > 0 {
                if tid < s {
                    sh_red_data(tid) = reduce(sh_red_data(tid), sh_red_data(tid + s));
                }

                acc.barrier();
                s >>= 1;
            }

            if tid == 0 {
                blocks_red_gpu(work_item.bidx()) = sh_red_data(0);
            }
        });

        acc.sync();
        dev.transfer(reduction_buffers_.buffer_gpu, reduction_buffers_.buffer_host);

        range(0, nblocks, |i| {
            red = reduce(red, blocks_red_host(i));
        });
    }

    red
}

fn @reduce_aabb(n: i32, b: AABB, reduce: fn(AABB, AABB) -> AABB, body: fn(i32) -> AABB) -> AABB {
    let dev = device();
    let acc = accelerator(device_id);
    let block_size = (64, 1, 1);
    let nblocks = round_up(n, block_size(0));
    let grid_size = (nblocks, 1, 1);
    let mut red = b;

    if n > 0 {
        reserve_reduction_buffers(nblocks, sizeof[AABB]());
        let blocks_red_host = get_array_of_aabb(reduction_buffers_.buffer_host);
        let blocks_red_gpu = get_array_of_aabb_accelerator(reduction_buffers_.buffer_gpu);

        acc.exec(grid_size, block_size, |work_item| {
            let sh_red_data = reserve_shared[AABB](block_size(0));
            let tid = work_item.tidx();
            let index = work_item.bidx() * work_item.bdimx() + work_item.tidx();

            if index < n {
                sh_red_data(tid) = body(index);
            } else {
                sh_red_data(tid) = b;
            }

            acc.barrier();

            let mut s = work_item.bdimx() >> 1;
            while s > 0 {
                if tid < s {
                    sh_red_data(tid) = reduce(sh_red_data(tid), sh_red_data(tid + s));
                }

                acc.barrier();
                s >>= 1;
            }

            if tid == 0 {
                blocks_red_gpu(work_item.bidx()) = sh_red_data(0);
            }
        });

        acc.sync();
        dev.transfer(reduction_buffers_.buffer_gpu, reduction_buffers_.buffer_host);

        range(0, nblocks, |i| {
            red = reduce(red, blocks_red_host(i));
        });
    }

    red
}

fn @get_neighborhood_aabb_buffer(nbh: Neighborhood) -> &[1][real_t] { get_array_of_reals_accelerator(nbh.aabbs_dev) }

fn get_neighborlist_index(particle_index: i32, neighbor_index: i32, grid: Grid) -> i32 {
    grid.particle_capacity * neighbor_index + particle_index
}
