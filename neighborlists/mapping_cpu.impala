fn @is_nvvm() -> bool { false }
fn @is_cuda() -> bool { false }
fn @is_opencl() -> bool { false }
fn @is_amdgpu() -> bool { false }
fn @has_ldg() -> bool { false }

extern "C" {
    fn rv_align(&[i8], i32) -> &[i8];
    fn rv_shuffle(real_t, i32) -> real_t;
}

fn @shuffle(x: real_t, srcLane: i32, laneOffset: i32) -> real_t {
    rv_shuffle(x, laneOffset)
}

fn @align(ptr: &[i8], alignment: i32) -> &[i8] {
    rv_align(ptr, alignment)
}

/*
fn @shuffle(x: real_t, src_lane: i32, laneOffset: i32) -> real_t {
    rv_shuffle(x as f32, laneOffset) as real_t
}

fn @align(ptr: &[i8], alignment: i32) -> &[i8] {
    rv_align(ptr as &i8, alignment) as &[i8]
}
*/

fn add_iterator(iterator: &mut i64) -> () {
    (*iterator)++;
}

fn @accelerator_allocate(size: i32) -> Buffer {
    Buffer {
        device: 0,
        data: 0 as &[i8],
        size: 0 as i64
    }
}

fn @accelerator_allocate_3d_arrays(N: i32) -> Array3D {
    null_3d_array()
}

fn @transfer_between_devices(source: Buffer, destination: Buffer) -> () {}
fn @transfer_3d_arrays_between_devices(source: Array3D, destination: Array3D) -> () {}

fn loop_accelerator(grid: Grid, body: fn(i32, &mut[i32], i32, i32) -> ()) -> () {
    let cell_sizes = get_array_of_i32(grid.cell_sizes_cpu);
    let neighbors_sizes = get_array_of_i32(grid.neighbors_sizes_cpu);
    let neighborlists = get_array_of_i32(grid.neighborlists_cpu);
    let neighborlist_capacity = grid.neighborlist_capacity;

    outer_loop_cpu(0, grid.nparticles, |particle_index| {
        if is_mask_true(get_ghost_mask(particle_index, grid)) {
            let nb_list_offset = neighborlist_capacity * particle_index;
            let nb_list_size = neighbors_sizes(particle_index);

            @@body(particle_index, neighborlists, nb_list_size, nb_list_offset);
        }
    });
}

fn @get_cell_particles(grid: Grid) -> &mut[i32] {
    get_array_of_i32(grid.cell_particles_cpu)
}

fn @get_cell_sizes(grid: Grid) -> &mut[i32] {
    get_array_of_i32(grid.cell_sizes_cpu)
}

fn @get_particles_cell(grid: Grid) -> &mut[i32] {
    get_array_of_i32(grid.particles_cell_cpu)
}

fn @get_neighborlist_index(particle_index: i32, neighbor_index: i32, grid: Grid) -> i32 {
    grid.neighborlist_capacity * particle_index + neighbor_index
}

fn @get_neighbors_sizes(grid: Grid) -> &mut[i32] {
    get_array_of_i32(grid.neighbors_sizes_cpu)
}

fn @get_number_of_neighbors(cluster_index: i32, grid: Grid) -> i32 {
    get_i32(cluster_index, grid.neighbors_sizes_cpu)
}

fn @get_neighborlists(grid: Grid) -> &mut[i32] {
    get_array_of_i32(grid.neighborlists_cpu)
}

fn @get_masses(grid: Grid) -> &mut[real_t] {
    get_array_of_reals(grid.masses_cpu)
}

fn @get_position(i: i32, grid: Grid) -> Vector3D {
    get_vector_from_3d_arrays(i, grid.positions_cpu)
}

fn @set_position(i: i32, grid: Grid, position: Vector3D) -> () {
    set_3d_arrays(i, grid.positions_cpu, position)
}

fn @get_velocity(i: i32, grid: Grid) -> Vector3D {
    get_vector_from_3d_arrays(i, grid.velocities_cpu)
}

fn @set_velocity(i: i32, grid: Grid, velocity: Vector3D) -> () {
    set_3d_arrays(i, grid.velocities_cpu, velocity)
}

fn @get_force(i: i32, grid: Grid) -> Vector3D {
    get_vector_from_3d_arrays(i, grid.forces_cpu)
}

fn @set_force(i: i32, grid: Grid, force: Vector3D) -> () {
    set_3d_arrays(i, grid.forces_cpu, force)
}

fn @reset_force(i: i32, grid: Grid) -> () {
    set_3d_arrays(i, grid.forces_cpu, Vector3D {x: 0.0 as real_t, y: 0.0 as real_t, z: 0.0 as real_t});
}

fn @add_to_force(i: i32, grid: Grid, dF_x: real_t, dF_y: real_t, dF_z: real_t) -> () {
    let mut force = get_vector_from_3d_arrays(i, grid.forces_cpu);

    force.x += dF_x;
    force.y += dF_y;
    force.z += dF_z;

    set_3d_arrays(i, grid.forces_cpu, force);
}

fn @get_ghost_mask(cluster_index: i32, grid: Grid) -> mask_t {
    get_mask_t(cluster_index, grid.ghost_mask_cpu)
}

fn alloc_comm_offsets(comm_offsets: &mut CommOffsets, world_size: i32, neighs: i32, send_capacity: i32, recv_capacity: i32) -> () {
    let null_buf = Buffer {
        device: 0,
        data: 0 as &[i8],
        size: 0 as i64
    };

    *comm_offsets = CommOffsets {
        // Host send data
        send_rank_offsets: null_buf,
        send_starts: null_buf,
        send_sizes: null_buf,
        send_offsets: null_buf,
        send_capacity: 0,
        send_noffsets: 0,

        // Host receive data
        recv_rank_offsets: null_buf,
        recv_starts: null_buf,
        recv_sizes: null_buf,
        recv_offsets: null_buf,
        recv_capacity: 0,
        recv_noffsets: 0,

        // Accelerator send data
        send_buffer_accelerator: null_buf,
        send_rank_offsets_accelerator: null_buf,
        send_starts_accelerator: null_buf,
        send_sizes_accelerator: null_buf,
        send_offsets_accelerator: null_buf,

        // Accelerator receive data
        recv_buffer_accelerator: null_buf,
        recv_rank_offsets_accelerator: null_buf,
        recv_starts_accelerator: null_buf,
        recv_sizes_accelerator: null_buf,
        recv_offsets_accelerator: null_buf,

        // Number of neighbor ranks
        neighs: 0
    };
}

// Build offsets used by scatter and gather kernels on GPU
fn build_comm_offsets(
    world_size: i32,
    rank: i32,
    grid: &Grid,
    comm_offsets: &mut CommOffsets) -> () {}

fn release_comm_offsets(comm_offsets: CommOffsets) -> () {}

fn gather_ghost_layer_cells(comm_buffer: Buffer, comm_offsets: &CommOffsets, grid: Grid) -> () {}

fn scatter_ghost_layer_cells(comm_buffer: Buffer, comm_offsets: &CommOffsets, grid: Grid) -> () {}

// Pack ghost layer cells in the CPU
fn pack_ghost_layer_cells(
    comm_buffer: Buffer,
    grid: Grid,
    begin_x: i32,
    begin_y: i32,
    begin_z: i32,
    end_x: i32,
    end_y: i32,
    end_z: i32) -> () {

    let cell_particles = get_array_of_i32(grid.cell_particles_cpu);
    let cell_sizes = get_array_of_i32(grid.cell_sizes_cpu);
    let cell_capacity = grid.cell_capacity;
    let elem_size = sizeof[real_t]();
    let mut buffer_ptr = 0;

    map_over_grid_subdomain(grid, begin_x, begin_y, begin_z, end_x, end_y, end_z, range, range, range, |flat_index, index| {
        let cell_size = cell_sizes(flat_index);
        let cell_offset = get_cell_offset(flat_index, grid);

        range(0, cell_size, |cell_particle_index| {
            let particle_index = cell_particles(cell_offset + cell_particle_index);
            copy_3d_arrays_to_buffer(grid.positions_cpu, particle_index * elem_size, comm_buffer, buffer_ptr, elem_size);
            buffer_ptr += 3 * elem_size;
        });
    });
}

// Unpack ghost layer cells in the CPU
fn unpack_ghost_layer_cells(
    comm_buffer: Buffer,
    grid: Grid,
    begin_x: i32,
    begin_y: i32,
    begin_z: i32,
    end_x: i32,
    end_y: i32,
    end_z: i32) -> () {

    let cell_particles = get_array_of_i32(grid.cell_particles_cpu);
    let cell_sizes = get_array_of_i32(grid.cell_sizes_cpu);
    let cell_capacity = grid.cell_capacity;
    let elem_size = sizeof[real_t]();
    let mut buffer_ptr = 0;

    map_over_grid_subdomain(grid, begin_x, begin_y, begin_z, end_x, end_y, end_z, range, range, range, |flat_index, index| {
        let cell_size = cell_sizes(flat_index);
        let cell_offset = get_cell_offset(flat_index, grid);

        range(0, cell_size, |cell_particle_index| {
            let particle_index = cell_particles(cell_offset + cell_particle_index);
            copy_buffer_to_3d_arrays(comm_buffer, buffer_ptr, grid.positions_cpu, particle_index * elem_size, elem_size);
            buffer_ptr += 3 * elem_size;
        });
    });
}

// Communication buffer sizes
fn get_comm_buffer_sizes(neighs: i32, max_send_particles: i32, max_recv_particles: i32) -> (i32, i32) {
    (max_send_particles * 3, max_recv_particles * 3)
}

// Start positions for communication buffer
fn get_comm_buffer_starts(exchange_rank: i32, comm_offsets: &CommOffsets) -> (i32, i32) { (0, 0) }
