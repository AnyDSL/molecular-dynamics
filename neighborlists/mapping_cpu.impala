fn @is_nvvm() -> bool { false }
fn @is_cuda() -> bool { false }
fn @is_opencl() -> bool { false }
fn @is_amdgpu() -> bool { false }
fn @has_ldg() -> bool { false }

extern "C" {
    fn rv_align(&[i8], i32) -> &[i8];
    fn rv_shuffle(real_t, i32) -> real_t;
}

fn @shuffle(x: real_t, srcLane: i32, laneOffset: i32) -> real_t {
    rv_shuffle(x, laneOffset)
}

fn @align(ptr: &[i8], alignment: i32) -> &[i8] {
    rv_align(ptr, alignment)
}

fn @accelerator_allocate(size: i32) -> Buffer {
    Buffer {
        device: 0,
        data: 0 as &[i8],
        size: 0 as i64
    }
}

fn @accelerator_allocate_3d_arrays(N: i32) -> Array3D {
    null_3d_array()
}

fn @transfer_between_devices(source: Buffer, destination: Buffer, size: i32) -> () {}
fn @transfer_3d_arrays_between_devices(source: Array3D, destination: Array3D, N: i32) -> () {}

fn loop_accelerator(accelerator_grid: AcceleratorGrid, size: i32, body: fn(i32, i32, i32, i32, i32, &mut[i32]) -> ()) -> () {
    for i in outer_loop_cpu(0, accelerator_grid.total_number_of_clusters) { 
        let cluster_size = get_cluster_size();
        let begin = i * cluster_size;
        let number_of_neighbors = get_number_of_neighbors(i, accelerator_grid);
        let neighborlist_offset = get_neighborlist_offset(i, accelerator_grid);
        let neighborlists = get_neighborlists(accelerator_grid);

        for j in vectorize(cluster_size) {
            body(i, begin, j, number_of_neighbors, neighborlist_offset, neighborlists);
        }
    }
}

fn @get_number_of_neighbors(cluster_index: i32, accelerator_grid: AcceleratorGrid) -> i32 {
    get_i32(cluster_index, accelerator_grid.neighbors_per_cluster_cpu)
}

fn @get_neighborlist_offset(cluster_index: i32, accelerator_grid: AcceleratorGrid) -> i32 {
    get_i32(cluster_index, accelerator_grid.neighborlist_offsets_cpu)
}

fn @get_neighborlists(accelerator_grid: AcceleratorGrid) -> &mut[i32] {
    get_array_of_i32(accelerator_grid.neighborlists_cpu)
}

fn @get_masses(accelerator_grid: AcceleratorGrid) -> &mut[real_t] {
    get_array_of_reals(accelerator_grid.masses_cpu)
}

fn @get_position(i: i32, accelerator_grid: AcceleratorGrid) -> Vector3D {
    get_vector_from_3d_arrays(i, accelerator_grid.positions_cpu)
}

fn @set_position(i: i32, accelerator_grid: AcceleratorGrid, position: Vector3D) -> () {
    set_3d_arrays(i, accelerator_grid.positions_cpu, position)
}

fn @get_velocity(i: i32, accelerator_grid: AcceleratorGrid) -> Vector3D {
    get_vector_from_3d_arrays(i, accelerator_grid.velocities_cpu)
}

fn @set_velocity(i: i32, accelerator_grid: AcceleratorGrid, velocity: Vector3D) -> () {
    set_3d_arrays(i, accelerator_grid.velocities_cpu, velocity)
}

fn @get_force(i: i32, accelerator_grid: AcceleratorGrid) -> Vector3D {
    get_vector_from_3d_arrays(i, accelerator_grid.forces_cpu)
}

fn @set_force(i: i32, accelerator_grid: AcceleratorGrid, force: Vector3D) -> () {
    set_3d_arrays(i, accelerator_grid.forces_cpu, force)
}

fn @reset_force(i: i32, accelerator_grid: AcceleratorGrid) -> () {
    set_3d_arrays(i, accelerator_grid.forces_cpu, Vector3D {x: 0.0 as real_t, y: 0.0 as real_t, z: 0.0 as real_t});
}

fn @add_to_force(i: i32, accelerator_grid: AcceleratorGrid, dF_x: real_t, dF_y: real_t, dF_z: real_t) -> () {
    let mut force = get_vector_from_3d_arrays(i, accelerator_grid.forces_cpu);
    force.x += dF_x;
    force.y += dF_y;
    force.z += dF_z;
    set_3d_arrays(i, accelerator_grid.forces_cpu, force);
}

fn @get_mask_value(i: i32, accelerator_grid: AcceleratorGrid) -> bool {
    get_bool(i, accelerator_grid.interaction_mask_cpu)
}

fn alloc_comm_offsets(world_size: i32, neighs: i32, send_capacity: i32, recv_capacity: i32) -> CommOffsets {
    let null_buf = Buffer {
        device: 0,
        data: 0 as &[i8],
        size: 0 as i64
    };

    let null_offset_buffers = CommOffsetBuffers {
        buffer_gpu: null_buf,
        rank_offsets: null_buf,
        start_buf: null_buf,
        sizes: null_buf,
        offsets: null_buf,
        rank_offsets_gpu: null_buf,
        start_buf_gpu: null_buf,
        sizes_gpu: null_buf,
        offsets_gpu: null_buf,
        capacity: 0,
        noffsets: 0,
        length: 0
    };

    CommOffsets {
        send_buffers: null_offset_buffers,
        recv_buffers: null_offset_buffers,
        neighs: 0
    }
}

fn release_comm_offsets(comm_offsets: CommOffsets) -> () {}

fn build_comm_offsets_accelerator(grid: &Grid, accelerator_grid: &AcceleratorGrid) -> () {}

// Pack ghost layer cells in the CPU
fn pack_ghost_layer_cells(
  comm_buffer: Buffer,
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  begin_x: i32,
  begin_y: i32,
  begin_z: i32,
  end_x: i32,
  end_y: i32,
  end_z: i32) -> () {

  let mut buffer_ptr = 0;

  for cell, index in
      map_over_grid_subdomain(
        grid,
        begin_x, begin_y, begin_z,
        end_x, end_y, end_z,
        range, range, range) {

    let flat_index = flatten_index(index, grid);
    let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
    let offset = cell_offsets(flat_index);
    let nparticles = cell.size;

    if(nparticles > 0) {
      buffer_ptr += copy_3d_arrays_to_buffer(
        accelerator_grid.positions_cpu, offset * sizeof[real_t](),
        comm_buffer, buffer_ptr,
        nparticles * sizeof[real_t]());
    }
  }
}

// Unpack ghost layer cells in the CPU
fn unpack_ghost_layer_cells(
  comm_buffer: Buffer,
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  begin_x: i32,
  begin_y: i32,
  begin_z: i32,
  end_x: i32,
  end_y: i32,
  end_z: i32) -> () {

  let mut buffer_ptr = 0;

  for cell, index in
      map_over_grid_subdomain(
        grid,
        begin_x, begin_y, begin_z,
        end_x, end_y, end_z,
        range, range, range) {

    let flat_index = flatten_index(index, grid);
    let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
    let offset = cell_offsets(flat_index);
    let nparticles = cell.size;

    if(nparticles > 0) {
      buffer_ptr += copy_buffer_to_3d_arrays(
        comm_buffer, buffer_ptr,
        accelerator_grid.positions_cpu, offset * sizeof[real_t](),
        nparticles * sizeof[real_t]());
    }
  }
}

// Synchronize ghost layer cells with neighbor ranks
fn synchronize_ghost_layer_cells(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mut request: MPI_Request;
  let mut status: MPIStatus;

  let rank_send_ptr = bitcast[&mut[i32]](rank_send_particles.data);
  let rank_recv_ptr = bitcast[&mut[i32]](rank_recv_particles.data);

  resize_comm_buffers(max_send_particles * 3, max_recv_particles * 3);

  for exchange_rank,
      send_begin_x, send_begin_y, send_begin_z,
      send_end_x, send_end_y, send_end_z,
      recv_begin_x, recv_begin_y, recv_begin_z,
      recv_end_x, recv_end_y, recv_end_z in
      communication_nodes(world_size, world_rank, grid) {

    if(rank_send_ptr(exchange_rank) > 0) {
      pack_ghost_layer_cells(
        comm_send_buffer, grid, accelerator_grid,
        send_begin_x, send_begin_y, send_begin_z,
        send_end_x, send_end_y, send_end_z);
    }

    if(rank_recv_ptr(exchange_rank) > 0) {
      mpih.irecv(
        bitcast[&mut[real_t]](comm_recv_buffer.data) as MPI_MutBuf,
        rank_recv_ptr(exchange_rank) * 3,
        mpih.double_t, exchange_rank, 0, mpih.comms.world, &mut request);
    }

    if(rank_send_ptr(exchange_rank) > 0) {
      mpih.send(
        bitcast[&mut[real_t]](comm_send_buffer.data) as MPI_MutBuf,
        rank_send_ptr(exchange_rank) * 3,
        mpih.double_t, exchange_rank, 0, mpih.comms.world);
    }

    if(rank_recv_ptr(exchange_rank) > 0) {
      mpih.wait(&mut request, &mut status);

      unpack_ghost_layer_cells(
        comm_recv_buffer, grid, accelerator_grid,
        recv_begin_x, recv_begin_y, recv_begin_z,
        recv_end_x, recv_end_y, recv_end_z);
    }
  }
}
