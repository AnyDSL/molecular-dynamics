fn @is_x86() -> bool { false }
fn @is_sse() -> bool { false }
fn @is_avx() -> bool { false }
fn @is_avx2() -> bool { false }

// In the GPU, we can either make each thread process a different cluster, or each thread
// process different particles in the same cluster, if this option is true we use the first
// approach, if this option is false we use the second one
fn @accelerator_split_clusters() -> bool { true }

fn @nvvm_shfldown_i32(var: i32, delta: i32, width: i32) -> i32 {
    let mut res: i32;
    let warp_size = 32; 
    let c = ((warp_size - width) << 8) | 0x1f;
    asm("shfl.down.b32 $0, $1, $2, $3;" : "=r"(res) : "r"(var), "r"(delta as u32), "r"(c));
    res 
}

fn @nvvm_shfldown_f64(var: f64, delta: i32, width: i32) -> f64 {
    let mut res = var;
    let mut lo: u32;
    let mut hi: u32;
    asm("mov.b64 { $0, $1 }, $2;" : "=r"(lo), "=r"(hi) : "d"(res));
    hi = nvvm_shfldown_i32(hi as i32, delta, width) as u32;
    lo = nvvm_shfldown_i32(lo as i32, delta, width) as u32;
    asm("mov.b64 $0, { $1, $2 };" : "=d"(res) : "r"(lo), "r"(hi));
    res 
}

fn @nvvm_shfl_i32(var: i32, src_lane: i32, width: i32) -> i32 {
    let mut res: i32;
    let warp_size = 32; 
    let c = ((warp_size - width) << 8) | 0x1f;
    asm("shfl.idx.b32 $0, $1, $2, $3;" : "=r"(res) : "r"(var), "r"(src_lane as u32), "r"(c));
    res 
}

fn @nvvm_shfl_f64(var: f64, src_lane: i32, width: i32) -> f64 {
    let mut res = var;
    let mut lo: u32;
    let mut hi: u32;
    asm("mov.b64 { $0, $1 }, $2;" : "=r"(lo), "=r"(hi) : "d"(res));
    hi = nvvm_shfl_i32(hi as i32, src_lane, width) as u32;
    lo = nvvm_shfl_i32(lo as i32, src_lane, width) as u32;
    asm("mov.b64 $0, { $1, $2 };" : "=d"(res) : "r"(lo), "r"(hi));
    res 
}


fn @shuffle(x: real_t, src_lane: i32, laneOffset: i32) -> real_t {
    nvvm_shfl_f64(x, src_lane, get_cluster_size())
}

fn @align(ptr: &[i8], alignment: i32) -> &[i8] {
    ptr
}

fn @accelerator_allocate(size: i32) -> Buffer {
    let acc = accelerator(device_id);
    acc.alloc(size)
}

fn @accelerator_allocate_3d_arrays(size: i32) -> Array3D {
    let acc = accelerator(device_id);
    allocate_3d_arrays(size, acc.alloc)
}

fn @transfer_between_devices(source: Buffer, destination: Buffer) -> () {
    copy(source, destination);
}

fn @transfer_3d_arrays_between_devices(source: Array3D, destination: Array3D) -> () {
    _transfer_3d_arrays_between_devices(source, destination, N);
}

fn loop_accelerator(accelerator_grid: AcceleratorGrid, body: fn(i32, i32, i32, i32, i32, &mut[1][i32]) -> ()) -> () {
    let cluster_size = get_cluster_size();
    let acc = accelerator(device_id);

    if accelerator_split_clusters() {
        let grid = (accelerator_grid.total_number_of_clusters * cluster_size, 1, 1);
        let block = (cluster_size, 1, 1);

        for work_item in acc.exec(grid, block) {
            let i = work_item.bidx();
            let neighborlists = get_neighborlists(accelerator_grid);

            if i < accelerator_grid.total_number_of_clusters {
                let j = work_item.tidx();
                let begin = i * cluster_size;
                let number_of_neighbors = get_number_of_neighbors(i, accelerator_grid);
                let neighborlist_offset = get_neighborlist_offset(i, accelerator_grid);

                @@body(i, begin, j, number_of_neighbors, neighborlist_offset, neighborlists);
            }
        }
    } else {
        let grid = (accelerator_grid.total_number_of_clusters * 128, 1, 1);
        let block = (128, 1, 1);

        for work_item in acc.exec(grid, block) {
            let i = work_item.bidx() * work_item.bdimx() + work_item.tidx();
            let neighborlists = get_neighborlists(accelerator_grid);

            if i < accelerator_grid.total_number_of_clusters {
                let begin = i * cluster_size;
                let number_of_neighbors = get_number_of_neighbors(i, accelerator_grid);
                let neighborlist_offset = get_neighborlist_offset(i, accelerator_grid);

                range(0, cluster_size, |j| {
                    @@body(i, begin, j, number_of_neighbors, neighborlist_offset, neighborlists);
                });
            }
        }
    }

    acc.sync();
}

fn @get_number_of_neighbors(cluster_index: i32, accelerator_grid: AcceleratorGrid) -> i32 {
    get_i32_accelerator(cluster_index, accelerator_grid.neighbors_per_cluster_accelerator)
}

fn @get_neighborlist_offset(cluster_index: i32, accelerator_grid: AcceleratorGrid) -> i32 {
    get_i32_accelerator(cluster_index, accelerator_grid.neighborlist_offsets_accelerator)
}

fn @get_neighborlists(accelerator_grid: AcceleratorGrid) -> &mut[1][i32] {
    get_array_of_i32_accelerator(accelerator_grid.neighborlists_accelerator)
}

fn @get_masses(accelerator_grid: AcceleratorGrid) -> &mut[1][real_t] {
    get_array_of_reals_accelerator(accelerator_grid.masses_accelerator)
}

fn @get_position(i: i32, accelerator_grid: AcceleratorGrid) -> Vector3D {
    get_vector_from_3d_arrays_accelerator(i, accelerator_grid.positions_accelerator)
}

fn @set_position(i: i32, accelerator_grid: AcceleratorGrid, position: Vector3D) -> () {
    set_3d_arrays_accelerator(i, accelerator_grid.positions_accelerator, position)
}

fn @get_velocity(i: i32, accelerator_grid: AcceleratorGrid) -> Vector3D {
    get_vector_from_3d_arrays_accelerator(i, accelerator_grid.velocities_accelerator)
}

fn @set_velocity(i: i32, accelerator_grid: AcceleratorGrid, velocity: Vector3D) -> () {
    set_3d_arrays_accelerator(i, accelerator_grid.velocities_accelerator, velocity)
}

fn @get_force(i: i32, accelerator_grid: AcceleratorGrid) -> Vector3D {
    get_vector_from_3d_arrays_accelerator(i, accelerator_grid.forces_accelerator)
}

fn @set_force(i: i32, accelerator_grid: AcceleratorGrid, force: Vector3D) -> () {
    set_3d_arrays_accelerator(i, accelerator_grid.forces_accelerator, force)
}

fn @reset_force(i: i32, accelerator_grid: AcceleratorGrid) -> () {
    set_3d_arrays_accelerator(i, accelerator_grid.forces_accelerator, Vector3D {x: 0.0 as real_t, y: 0.0 as real_t, z: 0.0 as real_t});
}

fn @add_to_force(i: i32, accelerator_grid: AcceleratorGrid, dF_x: real_t, dF_y: real_t, dF_z: real_t) -> () {
    let mut force = get_vector_from_3d_arrays_accelerator(i, accelerator_grid.forces_accelerator);
    force.x += dF_x;
    force.y += dF_y;
    force.z += dF_z;
    set_3d_arrays_accelerator(i, accelerator_grid.forces_accelerator, force);
}

fn @get_mask(i: i32, accelerator_grid: AcceleratorGrid) -> mask_t {
    get_mask_t_accelerator(i, accelerator_grid.interaction_mask_accelerator)
}

fn alloc_comm_offsets(comm_offsets: &mut CommOffsets, world_size: i32, neighs: i32, send_capacity: i32, recv_capacity: i32) -> () {
    let null_buf = Buffer {
        device: 0,
        data: 0 as &[i8],
        size: 0 as i64
    };

    if world_size > 1 && neighs > 0 && send_capacity > 0 && recv_capacity > 0 {
        *comm_offsets = CommOffsets {
            // Host send data
            send_rank_offsets: alloc_unaligned_cpu(world_size * sizeof[i32]()),
            send_starts: alloc_unaligned_cpu(send_capacity * sizeof[i32]()),
            send_sizes: alloc_unaligned_cpu(send_capacity * sizeof[i32]()),
            send_offsets: alloc_unaligned_cpu(send_capacity * sizeof[i32]()),
            send_capacity: send_capacity,
            send_noffsets: 0,

            // Host receive data
            recv_rank_offsets: alloc_unaligned_cpu(world_size * sizeof[i32]()),
            recv_starts: alloc_unaligned_cpu(recv_capacity * sizeof[i32]()),
            recv_sizes: alloc_unaligned_cpu(recv_capacity * sizeof[i32]()),
            recv_offsets: alloc_unaligned_cpu(recv_capacity * sizeof[i32]()),
            recv_capacity: recv_capacity,
            recv_noffsets: 0,

            // Accelerator send data
            send_buffer_accelerator: null_buf,
            send_rank_offsets_accelerator: accelerator_allocate(world_size * sizeof[i32]()),
            send_starts_accelerator: accelerator_allocate(send_capacity * sizeof[i32]()),
            send_sizes_accelerator: accelerator_allocate(send_capacity * sizeof[i32]()),
            send_offsets_accelerator: accelerator_allocate(send_capacity * sizeof[i32]()),

            // Accelerator receive data
            recv_buffer_accelerator: null_buf,
            recv_rank_offsets_accelerator: accelerator_allocate(world_size * sizeof[i32]()),
            recv_starts_accelerator: accelerator_allocate(recv_capacity * sizeof[i32]()),
            recv_sizes_accelerator: accelerator_allocate(recv_capacity * sizeof[i32]()),
            recv_offsets_accelerator: accelerator_allocate(recv_capacity * sizeof[i32]()),

            // Number of neighbor ranks
            neighs: neighs
        };
    }
}

// Build offsets used by scatter and gather kernels on GPU
fn build_comm_offsets(
    world_size: i32,
    rank: i32,
    grid: &Grid,
    accelerator_grid: &AcceleratorGrid,
    comm_offsets: &mut CommOffsets) -> () {

    let mut isend = 0;
    let mut irecv = 0;
    let mut ptr_send = 0;
    let mut ptr_recv = 0;
    let send_rank_offsets = get_array_of_i32(comm_offsets.send_rank_offsets);
    let recv_rank_offsets = get_array_of_i32(comm_offsets.recv_rank_offsets);
    let send_offsets = get_array_of_i32(comm_offsets.send_offsets);
    let recv_offsets = get_array_of_i32(comm_offsets.recv_offsets);
    let send_sizes = get_array_of_i32(comm_offsets.send_sizes);
    let recv_sizes = get_array_of_i32(comm_offsets.recv_sizes);
    let send_starts = get_array_of_i32(comm_offsets.send_starts);
    let recv_starts = get_array_of_i32(comm_offsets.recv_starts);

    if world_size > 1 {
        range(0, world_size, |r| {
            send_rank_offsets(r) = -1; 
            recv_rank_offsets(r) = -1; 
        });

        for exchange_rank,
            send_begin_x, send_begin_y, send_begin_z,
            send_end_x, send_end_y, send_end_z,
            recv_begin_x, recv_begin_y, recv_begin_z,
            recv_end_x, recv_end_y, recv_end_z in
            communication_nodes(world_size, rank, grid) {

            // Start of communication offsets for exchange rank
            send_rank_offsets(exchange_rank) = isend;
            recv_rank_offsets(exchange_rank) = irecv;

            for cell, index in
                map_over_grid_subdomain(
                    grid,
                    send_begin_x, send_begin_y, send_begin_z,
                    send_end_x, send_end_y, send_end_z,
                    range, range, range) {

                let flat_index = flatten_index(index, grid);
                let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);

                send_offsets(isend) = cell_offsets(flat_index);
                send_sizes(isend) = cell.size;
                send_starts(isend) = ptr_send;

                isend += 1;
                ptr_send += 3 * cell.size;
            }

            for cell, index in
                map_over_grid_subdomain(
                    grid,
                    recv_begin_x, recv_begin_y, recv_begin_z,
                    recv_end_x, recv_end_y, recv_end_z,
                    range, range, range) {

                let flat_index = flatten_index(index, grid);
                let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);

                recv_offsets(irecv) = cell_offsets(flat_index);
                recv_sizes(irecv) = cell.size;
                recv_starts(irecv) = ptr_recv;

                irecv += 1;
                ptr_recv += 3 * cell.size;
            }
        }

        comm_offsets.send_noffsets = isend;
        copy(comm_offsets.send_rank_offsets, comm_offsets.send_rank_offsets_accelerator);
        copy(comm_offsets.send_starts, comm_offsets.send_starts_accelerator);
        copy(comm_offsets.send_sizes, comm_offsets.send_sizes_accelerator);
        copy(comm_offsets.send_offsets, comm_offsets.send_offsets_accelerator);

        comm_offsets.recv_noffsets = irecv;
        copy(comm_offsets.recv_rank_offsets, comm_offsets.recv_rank_offsets_accelerator);
        copy(comm_offsets.recv_starts, comm_offsets.recv_starts_accelerator);
        copy(comm_offsets.recv_sizes, comm_offsets.recv_sizes_accelerator);
        copy(comm_offsets.recv_offsets, comm_offsets.recv_offsets_accelerator);
    }
}

fn release_comm_offsets(comm_offsets: CommOffsets) -> () {
    if comm_offsets.send_capacity > 0 {
        release(comm_offsets.send_buffer_accelerator);
        release(comm_offsets.send_rank_offsets);
        release(comm_offsets.send_starts);
        release(comm_offsets.send_sizes);
        release(comm_offsets.send_offsets);
        release(comm_offsets.send_rank_offsets_accelerator);
        release(comm_offsets.send_starts_accelerator);
        release(comm_offsets.send_sizes_accelerator);
        release(comm_offsets.send_offsets_accelerator);
    }

    if comm_offsets.recv_capacity > 0 {
        release(comm_offsets.recv_buffer_accelerator);
        release(comm_offsets.recv_rank_offsets);
        release(comm_offsets.recv_starts);
        release(comm_offsets.recv_sizes);
        release(comm_offsets.recv_offsets);
        release(comm_offsets.recv_rank_offsets_accelerator);
        release(comm_offsets.recv_starts_accelerator);
        release(comm_offsets.recv_sizes_accelerator);
        release(comm_offsets.recv_offsets_accelerator);
    }
}

fn @get_comm_send_starts_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.send_starts_accelerator)
}

fn @get_comm_send_offsets_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.send_offsets_accelerator)
}

fn @get_comm_send_sizes_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.send_sizes_accelerator)
}

fn @get_comm_recv_starts_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.recv_starts_accelerator)
}

fn @get_comm_recv_offsets_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.recv_offsets_accelerator)
}

fn @get_comm_recv_sizes_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.recv_sizes_accelerator)
}

fn gather_ghost_layer_cells(comm_buffer: Buffer, comm_offsets: CommOffsets, accelerator_grid: AcceleratorGrid) -> () {
    let acc = accelerator(device_id);
    let grid = (comm_offsets.send_noffsets * 64, 1, 1);
    let block = (64, 1, 1);
    let buffer_gpu = comm_offsets.send_buffer_accelerator;
    let noffsets = comm_offsets.send_noffsets;

    if noffsets > 0 {
        for work_item in acc.exec(grid, block) {
            let th_idx = work_item.bidx() * work_item.bdimx() + work_item.tidx();

            if th_idx < noffsets {
                let start = get_comm_send_starts_accelerator(th_idx, comm_offsets);
                let offset = get_comm_send_offsets_accelerator(th_idx, comm_offsets);
                let size = get_comm_send_sizes_accelerator(th_idx, comm_offsets);

                for i in range(0, size) {
                    let linear_idx = i * 3;
                    let pos = get_position(offset + i, accelerator_grid);

                    set_real_accelerator(start + linear_idx,     buffer_gpu, pos.x);
                    set_real_accelerator(start + linear_idx + 1, buffer_gpu, pos.y);
                    set_real_accelerator(start + linear_idx + 2, buffer_gpu, pos.z);
                }
            }
        }

        acc.sync();
        copy(buffer_gpu, comm_buffer);
    }
}

fn scatter_ghost_layer_cells(comm_buffer: Buffer, comm_offsets: CommOffsets, accelerator_grid: AcceleratorGrid) -> () {
    let acc = accelerator(device_id);
    let grid = (comm_offsets.recv_noffsets * 64, 1, 1);
    let block = (64, 1, 1);
    let buffer_gpu = comm_offsets.recv_buffer_accelerator;
    let noffsets = comm_offsets.recv_noffsets;

    if noffsets > 0 {
        copy(comm_buffer, buffer_gpu);

        for work_item in acc.exec(grid, block) {
            let th_idx = work_item.bidx() * work_item.bdimx() + work_item.tidx();

            if th_idx < noffsets {
                let start = get_comm_recv_starts_accelerator(th_idx, comm_offsets);
                let offset = get_comm_recv_offsets_accelerator(th_idx, comm_offsets);
                let size = get_comm_recv_sizes_accelerator(th_idx, comm_offsets);

                for i in range(0, size) {
                    let linear_idx = i * 3;
                    let pos = Vector3D {
                        x: get_real_accelerator(start + linear_idx,     buffer_gpu),
                        y: get_real_accelerator(start + linear_idx + 1, buffer_gpu),
                        z: get_real_accelerator(start + linear_idx + 2, buffer_gpu),
                    };

                    set_position(offset + i, accelerator_grid, pos);
                }
            }
        }

        acc.sync();
    }
}

// Pack ghost layer cells (CPU only)
fn pack_ghost_layer_cells(
    comm_buffer: Buffer,
    grid: &mut Grid,
    accelerator_grid: AcceleratorGrid,
    begin_x: i32,
    begin_y: i32,
    begin_z: i32,
    end_x: i32,
    end_y: i32,
    end_z: i32) -> () {}

// Unpack ghost layer cells (CPU only)
fn unpack_ghost_layer_cells(
    comm_buffer: Buffer,
    grid: &mut Grid,
    accelerator_grid: AcceleratorGrid,
    begin_x: i32,
    begin_y: i32,
    begin_z: i32,
    end_x: i32,
    end_y: i32,
    end_z: i32) -> () {}

// Communication buffer sizes
fn get_comm_buffer_sizes(neighs: i32, max_send_particles: i32, max_recv_particles: i32) -> (i32, i32) {
    (neighs * max_send_particles * 3, neighs * max_recv_particles * 3)
}

// Start positions for communication buffers
fn get_comm_buffer_starts(exchange_rank: i32, comm_offsets: &CommOffsets) -> (i32, i32) {
    let send_rank_offsets = get_array_of_i32(comm_offsets.send_rank_offsets);
    let recv_rank_offsets = get_array_of_i32(comm_offsets.recv_rank_offsets);
    let send_starts = get_array_of_i32(comm_offsets.send_starts);
    let recv_starts = get_array_of_i32(comm_offsets.recv_starts);
    let recv_offset = recv_rank_offsets(exchange_rank);
    let send_offset = send_rank_offsets(exchange_rank);
    let recv_start = recv_starts(recv_offset);
    let send_start = send_starts(send_offset);

    (send_start, recv_start)
}
