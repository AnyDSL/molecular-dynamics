fn @is_x86() -> bool { false }
fn @is_sse() -> bool { false }
fn @is_avx() -> bool { false }
fn @is_avx2() -> bool { false }

fn @nvvm_shfldown_i32(var: i32, delta: i32, width: i32) -> i32 {
    let mut res: i32;
    let warp_size = 32; 
    let c = ((warp_size - width) << 8) | 0x1f;
    asm("shfl.down.b32 $0, $1, $2, $3;" : "=r"(res) : "r"(var), "r"(delta as u32), "r"(c));
    res 
}

fn @nvvm_shfldown_f64(var: f64, delta: i32, width: i32) -> f64 {
    let mut res = var;
    let mut lo: u32;
    let mut hi: u32;
    asm("mov.b64 { $0, $1 }, $2;" : "=r"(lo), "=r"(hi) : "d"(res));
    hi = nvvm_shfldown_i32(hi as i32, delta, width) as u32;
    lo = nvvm_shfldown_i32(lo as i32, delta, width) as u32;
    asm("mov.b64 $0, { $1, $2 };" : "=d"(res) : "r"(lo), "r"(hi));
    res 
}

fn @nvvm_shfl_i32(var: i32, src_lane: i32, width: i32) -> i32 {
    let mut res: i32;
    let warp_size = 32; 
    let c = ((warp_size - width) << 8) | 0x1f;
    asm("shfl.idx.b32 $0, $1, $2, $3;" : "=r"(res) : "r"(var), "r"(src_lane as u32), "r"(c));
    res 
}

fn @nvvm_shfl_f64(var: f64, src_lane: i32, width: i32) -> f64 {
    let mut res = var;
    let mut lo: u32;
    let mut hi: u32;
    asm("mov.b64 { $0, $1 }, $2;" : "=r"(lo), "=r"(hi) : "d"(res));
    hi = nvvm_shfl_i32(hi as i32, src_lane, width) as u32;
    lo = nvvm_shfl_i32(lo as i32, src_lane, width) as u32;
    asm("mov.b64 $0, { $1, $2 };" : "=d"(res) : "r"(lo), "r"(hi));
    res 
}

fn @shuffle(x: real_t, src_lane: i32, laneOffset: i32) -> real_t {
    nvvm_shfl_f64(x, src_lane, get_cluster_size())
}

fn @align(ptr: &[i8], alignment: i32) -> &[i8] {
    ptr
}

fn add_iterator(iterator: &mut i64) -> () {}

fn @accelerator_allocate(size: i32) -> Buffer {
    let acc = accelerator(device_id);
    acc.alloc(size)
}

fn @accelerator_allocate_3d_arrays(size: i32) -> Array3D {
    let acc = accelerator(device_id);
    allocate_3d_arrays(size, acc.alloc)
}

fn @transfer_between_devices(source: Buffer, destination: Buffer) -> () {
    copy(source, destination);
}

fn @transfer_3d_arrays_between_devices(source: Array3D, destination: Array3D) -> () {
    _transfer_3d_arrays_between_devices(source, destination);
}

fn loop_accelerator(grid: Grid, body: fn(i32, &mut[1][i32], i32, i32) -> ()) -> () {
    let cell_sizes = get_array_of_i32_accelerator(grid.cell_sizes_accelerator);
    let neighbors_sizes = get_array_of_i32_accelerator(grid.neighbors_sizes_accelerator);
    let neighborlists = get_array_of_i32_accelerator(grid.neighborlists_accelerator);
    let neighborlist_capacity = grid.neighborlist_capacity;
    let nparticles = grid.nparticles;

    let acc = accelerator(device_id);
    let grid_size = (nparticles * 32, 1, 1);
    let block_size = (32, 1, 1);

    acc.exec(grid_size, block_size, |work_item| {
        let particle_index = work_item.bidx() * work_item.bdimx() + work_item.tidx();

        if particle_index < nparticles {
            if is_mask_true(get_ghost_mask(particle_index, grid)) {
                let nb_list_offset = neighborlist_capacity * particle_index;
                let nb_list_size = neighbors_sizes(particle_index);

                @@body(particle_index, neighborlists, nb_list_size, nb_list_offset);
            }
        }
    });

    acc.sync();
}

fn @get_particles_cell(grid: Grid) -> &mut[1][i32] {
    get_array_of_i32_accelerator(grid.particles_cell_accelerator)
}

fn @get_cell_particles(grid: Grid) -> &mut[1][i32] {
    get_array_of_i32_accelerator(grid.cell_particles_accelerator)
}

fn @get_cell_sizes(grid: Grid) -> &mut[1][i32] {
    get_array_of_i32_accelerator(grid.cell_sizes_accelerator)
}

fn @get_neighborlist_index(particle_index: i32, neighbor_index: i32, grid: Grid) -> i32 {
    grid.particle_capacity * neighbor_index + particle_index
}

fn @get_neighbors_sizes(grid: Grid) -> &mut[1][i32] {
    get_array_of_i32_accelerator(grid.neighbors_sizes_accelerator)
}

fn @get_neighborlists(grid: Grid) -> &mut[1][i32] {
    get_array_of_i32_accelerator(grid.neighborlists_accelerator)
}

fn @get_masses(grid: Grid) -> &mut[1][real_t] {
    get_array_of_reals_accelerator(grid.masses_accelerator)
}

fn @get_position(i: i32, grid: Grid) -> Vector3D {
    get_vector_from_3d_arrays_accelerator(i, grid.positions_accelerator)
}

fn @set_position(i: i32, grid: Grid, position: Vector3D) -> () {
    set_3d_arrays_accelerator(i, grid.positions_accelerator, position)
}

fn @get_velocity(i: i32, grid: Grid) -> Vector3D {
    get_vector_from_3d_arrays_accelerator(i, grid.velocities_accelerator)
}

fn @set_velocity(i: i32, grid: Grid, velocity: Vector3D) -> () {
    set_3d_arrays_accelerator(i, grid.velocities_accelerator, velocity)
}

fn @get_force(i: i32, grid: Grid) -> Vector3D {
    get_vector_from_3d_arrays_accelerator(i, grid.forces_accelerator)
}

fn @set_force(i: i32, grid: Grid, force: Vector3D) -> () {
    set_3d_arrays_accelerator(i, grid.forces_accelerator, force)
}

fn @reset_force(i: i32, grid: Grid) -> () {
    set_3d_arrays_accelerator(i, grid.forces_accelerator, Vector3D {x: 0.0 as real_t, y: 0.0 as real_t, z: 0.0 as real_t});
}

fn @add_to_force(i: i32, grid: Grid, dF_x: real_t, dF_y: real_t, dF_z: real_t) -> () {
    let mut force = get_vector_from_3d_arrays_accelerator(i, grid.forces_accelerator);
    force.x += dF_x;
    force.y += dF_y;
    force.z += dF_z;
    set_3d_arrays_accelerator(i, grid.forces_accelerator, force);
}

fn @get_ghost_mask(i: i32, grid: Grid) -> mask_t {
    get_mask_t_accelerator(i, grid.ghost_mask_accelerator)
}

fn @get_comm_send_offsets_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.send_offsets_accelerator)
}

fn @get_comm_recv_offsets_accelerator(index: i32, comm_offsets: CommOffsets) -> i32 {
    get_i32_accelerator(index, comm_offsets.recv_offsets_accelerator)
}

fn gather_loop(comm_offsets: CommOffsets, body: fn(i32, Buffer, i32, i32, fn(i32, Buffer, real_t) -> ()) -> ()) -> () {
    let acc = accelerator(device_id);
    let grid_size = (comm_offsets.send_noffsets * 64, 1, 1);
    let block_size = (64, 1, 1);
    let buffer = comm_offsets.send_buffer_accelerator;
    let noffsets = comm_offsets.send_noffsets;

    if noffsets > 0 {
        for work_item in acc.exec(grid_size, block_size) {
            let th_idx = work_item.bidx() * work_item.bdimx() + work_item.tidx();

            if th_idx < noffsets {
                let offset = get_comm_send_offsets_accelerator(th_idx, comm_offsets);
                let start = th_idx * 3;

                @@body(th_idx, buffer, start, offset, set_real_accelerator);
            }
        }

        acc.sync();
        copy(buffer, comm_offsets.send_buffer);
    }
}

fn scatter_loop(comm_offsets: CommOffsets, body: fn(i32, Buffer, i32, i32, fn(i32, Buffer) -> real_t) -> ()) -> () {
    let acc = accelerator(device_id);
    let grid_size = (comm_offsets.recv_noffsets * 64, 1, 1);
    let block_size = (64, 1, 1);
    let buffer = comm_offsets.recv_buffer_accelerator;
    let noffsets = comm_offsets.recv_noffsets;

    if noffsets > 0 {
        copy_offset(comm_offsets.recv_buffer, 0, buffer, 0, buffer.size as i32);

        for work_item in acc.exec(grid_size, block_size) {
            let th_idx = work_item.bidx() * work_item.bdimx() + work_item.tidx();

            if th_idx < noffsets {
                let offset = get_comm_recv_offsets_accelerator(th_idx, comm_offsets);
                let start = th_idx * 3;

                @@body(th_idx, buffer, start, offset, get_real_accelerator);
            }
        }

        acc.sync();
    }
}
