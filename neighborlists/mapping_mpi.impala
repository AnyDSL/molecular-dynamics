/* Number of nodes in each dimension */
static mut node_dims: [i32 * 3];

/* Lookup table for ghost layer ranges */
static mut grid_transfer_ranges: [[i32 * 4] * 9];

/* Number of send and receive particles per rank */
static mut rank_send_particles: Buffer;
static mut rank_recv_particles: Buffer;

/* Maximum number of send and receive particles */
static mut max_send_particles: i32;
static mut max_recv_particles: i32;

/* Buffers for send and receive communications */
static mut comm_send_buffer: Buffer;
static mut comm_recv_buffer: Buffer;

/* Sizes for communication buffers */
static mut comm_send_buffer_size: i32;
static mut comm_recv_buffer_size: i32;

/* Timesteps to perform cell synchronization (ghost layer width) */
fn @get_sync_timesteps() -> i32 { 1 }

/* Must synchronize diagonals (true/false) */
fn @synchronize_diagonals() -> bool { false }

/* Flatten index for ranks (3D cartesian mapping) */
fn @flat_index(index: [i32 * 3], nx: i32, ny: i32) -> i32 {
  (index(2) * ny + index(1)) * nx + index(0)
}

/* Unflatten index for ranks (3D cartesian mapping) */
fn @unflat_index(index: i32, nx: i32, ny: i32, nz: i32) -> [i32 * 3] {
  [index % nx, (index / nx) % ny, index / (nx * ny)]
}

/* Mirrored communication (avoid deadlocks) */
fn @mirror_comm(
  xoffset: i32,
  yoffset: i32,
  zoffset: i32,
  body: fn(i32, i32, i32) -> ()) -> () {

  @@body(xoffset, yoffset, zoffset);
  @@body(-xoffset, -yoffset, -zoffset);
}

/* Initialize MPI and defined data structures for communication */
fn mpi_initialize(world_size: &mut i32, world_rank: &mut i32) -> () {
  let mpih = mpi();

  /* Initialize MPI */
  mpih.init();

  /* Set sizes to zero */
  comm_send_buffer_size = 0;
  comm_recv_buffer_size = 0;

  /* Get world size and rank for current node */
  mpih.comm_size(mpih.comms.world, world_size);
  mpih.comm_rank(mpih.comms.world, world_rank);

  /* Add allocation amount */
  add_mpi_allocation(sizeof[i32]() * *world_size * 2);

  /* Allocate buffer for storing number of send and receive particles */
  rank_send_particles = alloc_unaligned_cpu(sizeof[i32]() * *world_size);
  rank_recv_particles = alloc_unaligned_cpu(sizeof[i32]() * *world_size);
}

/* Finalize MPI and free data structures for communication */
fn mpi_finalize() -> () {
  let mpih = mpi();

  /* Release communication send buffer */
  if(comm_send_buffer_size > 0) {
    release_host(comm_send_buffer);
  }

  /* Release communication receive buffer */
  if(comm_recv_buffer_size > 0) {
    release_host(comm_recv_buffer);
  }

  /* Release buffer for storing number of send and receive particles */
  release_host(rank_send_particles);
  release_host(rank_recv_particles);

  /* Finalize MPI */
  mpih.finalize();
}

/* Resize communication buffers (if necessary) */
fn resize_comm_buffers(send_size: i32, recv_size: i32) -> () {
  /* If new send buffer size is higher than current one */
  if(send_size > comm_send_buffer_size) {
    /* Release communication send buffer */
    if(comm_send_buffer_size > 0) {
      add_mpi_allocation(-(sizeof[real_t]() * comm_send_buffer_size));
      release_host(comm_send_buffer);
    }

    /* Add allocation amount */
    add_mpi_allocation(sizeof[real_t]() * send_size);

    /* Allocate new buffer with new size and update size */
    comm_send_buffer = alloc_unaligned_cpu(sizeof[real_t]() * send_size);
    comm_send_buffer_size = send_size;
  }

  /* If new receive buffer size is higher than current one */
  if(recv_size > comm_recv_buffer_size) {
    /* Release communication receive buffer */
    if(comm_recv_buffer_size > 0) {
      add_mpi_allocation(-(sizeof[real_t]() * comm_recv_buffer_size));
      release_host(comm_recv_buffer);
    }

    /* Add allocation amount */
    add_mpi_allocation(sizeof[real_t]() * recv_size);

    /* Allocate new buffer with new size and update size */
    comm_recv_buffer = alloc_unaligned_cpu(sizeof[real_t]() * recv_size);
    comm_recv_buffer_size = recv_size;
  }
}

/* Communication pattern for 1D domain partitioning */
fn communication_nodes_1d(
  world_size: i32,
  rank: i32,
  grid: Grid,
  body: fn(i32, [i32 * 3], [i32 * 3], [i32 * 3], [i32 * 3]) -> ()) -> () {

  /* If rank is not the last one */
  if(rank < world_size - 1) {
    /* Communicate with next rank */
    @@body(
      rank + 1,
      [0, 0, grid.nz - get_sync_timesteps() * 2],
      [grid.nx, grid.ny, grid.nz - get_sync_timesteps()],
      [0, 0, grid.nz - get_sync_timesteps()],
      [grid.nx, grid.ny, grid.nz]
    );
  }

  /* If rank is not the first one */
  if(rank > 0) {
    /* Communicate with previous rank */
    @@body(
      rank - 1,
      [0, 0, get_sync_timesteps()],
      [grid.nx, grid.ny, get_sync_timesteps() * 2],
      [0, 0, 0],
      [grid.nx, grid.ny, get_sync_timesteps()]
    );
  }
}

/* Get current node bounding box for 1D domain partitioning */
fn @get_node_bounding_box_1d(
  world_size: i32,
  rank: i32,
  cell_spacing: real_t,
  grid_aabb: AABB) -> AABB {

  /* Delimiters for current node bounding box */
  let mut zmin: real_t;
  let mut zmax: real_t;

  /* If there are more nodes */
  if(world_size > 1) {
    /* Number of cells in z axis */
    let zcells = real_floor(
      (grid_aabb.max(2) - grid_aabb.min(2)) / cell_spacing
    ) as i32;

    /* Total length for cells in z axis */
    let zlength = (zcells / world_size) as real_t * cell_spacing;

    /* Calculate zmin and zmax based on rank value */
    zmin = grid_aabb.min(2) + zlength * (rank as real_t);
    zmax = grid_aabb.min(2) + zlength * ((rank + 1) as real_t);

    /* If this is not the first node, include ghost layer for lower z ranks */
    if(rank > 0) {
      zmin -= (get_sync_timesteps() as real_t) * cell_spacing;
    }

    /* If this is not the last node, include ghost layer for upper z ranks */
    if(rank < world_size - 1) {
      zmax += (get_sync_timesteps() as real_t) * cell_spacing;
    /* Otherwise, just increase cell spacing to maximum delimiter to
       reach any possible missing cells in the original bounding box */
    } else {
      zmax += cell_spacing;
    }
  /* Otherwise, just use original bounding box */
  } else {
    zmin = grid_aabb.min(2);
    zmax = grid_aabb.max(2);
  }

  AABB {
    min: [grid_aabb.min(0), grid_aabb.min(1), zmin],
    max: [grid_aabb.max(0), grid_aabb.max(1), zmax]
  }
}

/* Get send and receive transfer ranges for communications */
fn transfer_ranges(offset: i32, ncells: i32) -> [i32 * 4] {
  let mut send_begin: i32;
  let mut send_end: i32;
  let mut recv_begin: i32;
  let mut recv_end: i32;

  /* If offset is zero (same position), communication must comprise all
     cells in this axis (adjacent nodes) */
  if(offset == 0) {
    send_begin = 0;
    send_end = ncells;
    recv_begin = 0;
    recv_end = ncells;
  /* If offset is less than zero (lower position), communication must
     comprise only last cells in this axis */
  } else if(offset < 0) {
    send_begin = ncells - get_sync_timesteps() * 2;
    send_end = ncells - get_sync_timesteps();
    recv_begin = ncells - get_sync_timesteps();
    recv_end = ncells;
  /* If offset is greater than zero (lower position), communication must
     comprise only first cells in this axis */
  } else {
    send_begin = get_sync_timesteps();
    send_end = get_sync_timesteps() * 2;
    recv_begin = 0;
    recv_end = get_sync_timesteps();
  }

  [send_begin, send_end, recv_begin, recv_end]
}

/* Build lookup table for transfer ranges in all dimensions */
fn initialize_grid_transfer_ranges(grid: Grid) -> () {
  /* Go through all offsets (-1, 0, +1) */
  for i in range(-1, 2) {
    /* Get transfer ranges for x axis */
    let xranges = transfer_ranges(i, grid.nx);
    /* Get transfer ranges for y axis */
    let yranges = transfer_ranges(i, grid.ny);
    /* Get transfer ranges for z axis */
    let zranges = transfer_ranges(i, grid.nz);

    /* Go through ranges (send begin, send end, recv begin and recv end) */
    for j in range(0, 4) {
      /* Copy ranges to the table */
      grid_transfer_ranges(i + 1)(j) = xranges(j);
      grid_transfer_ranges(i + 4)(j) = yranges(j);
      grid_transfer_ranges(i + 7)(j) = zranges(j);
    }
  }
}

/* Generic communication pattern (include diagonals) */
fn communication_nodes_generic(
  world_size: i32,
  rank: i32,
  grid: Grid,
  body: fn(i32, [i32 * 3], [i32 * 3], [i32 * 3], [i32 * 3]) -> ()) -> () {

  /* Index of rank in the 3D cartesian grid */
  let rank_index = unflat_index(rank, node_dims(0), node_dims(1), node_dims(2));

  /* Has previous node in x axis */
  let xprev = rank_index(0) > 0;
  /* Has successor node in x axis */
  let xnext = rank_index(0) < node_dims(0) - 1;
  /* Has previous node in y axis */
  let yprev = rank_index(1) > 0;
  /* Has successor node in y axis */
  let ynext = rank_index(1) < node_dims(1) - 1;
  /* Has previous node in z axis */
  let zprev = rank_index(2) > 0;
  /* Has successor node in z axis */
  let znext = rank_index(2) < node_dims(2) - 1;

  /* Go through x possible offsets */
  for xoffset in range(-1, 1) {
    /* Go through y possible offsets */
    for yoffset in range(-1, 1 - xoffset) {
      /* Go through z possible offsets */
      for zoffset in range(-1, 2) {
        /* Number of zero offsets (diagonals include zero or one) */
        let nzeros = (xoffset == 0) as i32 +
                     (yoffset == 0) as i32 +
                     (zoffset == 0) as i32;

        //if(nzeros == 2 || (synchronize_diagonals() && nzeros < 3)) {

        /* If number of zeros is two, not a diagonal (adjacent node) */
        if(nzeros == 2) {
          /* Perform for original and mirrored offsets to avoid deadlocks */
          for xoff, yoff, zoff in mirror_comm(xoffset, yoffset, zoffset) {
            /* Check if node for communication is valid */
            if(
                ((xoff < 0 && xprev) || (xoff > 0 && xnext) || (xoff == 0)) &&
                ((yoff < 0 && yprev) || (yoff > 0 && ynext) || (yoff == 0)) &&
                ((zoff < 0 && zprev) || (zoff > 0 && znext) || (zoff == 0))
            ) {
              /* Exchange rank based on offsets */
              let exchange_rank = flat_index(
                [rank_index(0) + xoff, rank_index(1) + yoff, rank_index(2) + zoff],
                node_dims(0), node_dims(1)
              );

              /* Get x ranges on lookup table */
              let xranges = grid_transfer_ranges(xoff + 1);
              /* Get y ranges on lookup table */
              let yranges = grid_transfer_ranges(yoff + 4);
              /* Get z ranges on lookup table */
              let zranges = grid_transfer_ranges(zoff + 7);

              /* Build ranges based on vectors obtained from the lookup table */
              let send_begin = [xranges(0), yranges(0), zranges(0)];
              let send_end   = [xranges(1), yranges(1), zranges(1)];
              let recv_begin = [xranges(2), yranges(2), zranges(2)];
              let recv_end   = [xranges(3), yranges(3), zranges(3)];

              /* Performs communication with rank using the obtained ranges */
              @@body(exchange_rank, send_begin, send_end, recv_begin, recv_end);
            }
          }
        }
      }
    }
  }
}

/* Is this the last node in the dimension? */
fn last_dimension_node(dim: i32) -> bool {
  let mpih = mpi();
  let mut rank: i32;

  /* Get rank for current node */
  mpih.comm_rank(mpih.comms.world, &mut rank);

  /* Get rank cartesian index */
  let rank_index = unflat_index(rank, node_dims(0), node_dims(1), node_dims(2));

  /* Check if this is the last one in the dimension */
  rank_index(dim) == node_dims(dim) - 1
}

/* Default communication pattern (do not include diagonals) */
fn communication_nodes(
  world_size: i32,
  rank: i32,
  grid: Grid,
  body: fn(i32, [i32 * 3], [i32 * 3], [i32 * 3], [i32 * 3]) -> ()) -> () {

  /* Index of rank in the 3D cartesian grid */
  let rank_index = unflat_index(rank, node_dims(0), node_dims(1), node_dims(2));

  /* Has previous node in x axis */
  let xprev = rank_index(0) > 0;
  /* Has successor node in x axis */
  let xnext = rank_index(0) < node_dims(0) - 1;
  /* Has previous node in y axis */
  let yprev = rank_index(1) > 0;
  /* Has successor node in y axis */
  let ynext = rank_index(1) < node_dims(1) - 1;
  /* Has previous node in z axis */
  let zprev = rank_index(2) > 0;
  /* Has successor node in z axis */
  let znext = rank_index(2) < node_dims(2) - 1;

  if(xnext) {
    @@body(
      flat_index(
        [rank_index(0) + 1, rank_index(1), rank_index(2)],
        node_dims(0), node_dims(1)),
      [grid.nx - get_sync_timesteps() * 2, 0, 0],
      [grid.nx - get_sync_timesteps(), grid.ny, grid.nz],
      [grid.nx - get_sync_timesteps(), 0, 0],
      [grid.nx, grid.ny, grid.nz]
    );
  }

  if(xprev) {
    @@body(
      flat_index(
        [rank_index(0) - 1, rank_index(1), rank_index(2)],
        node_dims(0), node_dims(1)),
      [get_sync_timesteps(), 0, 0],
      [get_sync_timesteps() * 2, grid.ny, grid.nz],
      [0, 0, 0],
      [get_sync_timesteps(), grid.ny, grid.nz]
    );
  }

  if(ynext) {
    @@body(
      flat_index(
        [rank_index(0), rank_index(1) + 1, rank_index(2)],
        node_dims(0), node_dims(1)),
      [0, grid.ny - get_sync_timesteps() * 2, 0],
      [grid.nx, grid.ny - get_sync_timesteps(), grid.nz],
      [0, grid.ny - get_sync_timesteps(), 0],
      [grid.nx, grid.ny, grid.nz]
    );
  }

  if(yprev) {
    @@body(
      flat_index(
        [rank_index(0), rank_index(1) - 1, rank_index(2)],
        node_dims(0), node_dims(1)),
      [0, get_sync_timesteps(), 0],
      [grid.nx, get_sync_timesteps() * 2, grid.nz],
      [0, 0, 0],
      [grid.nx, get_sync_timesteps(), grid.nz]
    );
  }

  if(znext) {
    @@body(
      flat_index(
        [rank_index(0), rank_index(1), rank_index(2) + 1],
        node_dims(0), node_dims(1)),
      [0, 0, grid.nz - get_sync_timesteps() * 2],
      [grid.nx, grid.ny, grid.nz - get_sync_timesteps()],
      [0, 0, grid.nz - get_sync_timesteps()],
      [grid.nx, grid.ny, grid.nz]
    );
  }

  if(zprev) {
    @@body(
      flat_index(
        [rank_index(0), rank_index(1), rank_index(2) - 1],
        node_dims(0), node_dims(1)),
      [0, 0, get_sync_timesteps()],
      [grid.nx, grid.ny, get_sync_timesteps() * 2],
      [0, 0, 0],
      [grid.nx, grid.ny, get_sync_timesteps()]
    );
  }
}

fn get_node_config(
  world_size: i32,
  rank: i32,
  xcells: i32,
  ycells: i32,
  zcells: i32) -> [i32 * 3] {

  let mut gx = 1;
  let mut gy = 1;
  let mut gz = 1;
  let mut min_missing_factor = xcells * ycells * zcells;

  for i in range(1, world_size) {
    if(world_size % i == 0) {
      let rem_yz = world_size / i;

      for j in range(1, rem_yz) {
        if(rem_yz % j == 0) {
          let k = rem_yz / j;
          let missing_factor = xcells % i + ycells % j + zcells % k;

          if(min_missing_factor > missing_factor) {
            gx = i;
            gy = j;
            gz = k;
            min_missing_factor = missing_factor;
          }
        }
      }
    }
  }

  /*
  print_string("Node config: ");
  print_i32(gx);
  print_string(", ");
  print_i32(gy);
  print_string(", ");
  print_i32(gz);
  print_string("\n");
  */

  [gx, gy, gz]
}

fn @get_node_bounding_box(
  world_size: i32,
  rank: i32,
  cell_spacing: real_t,
  aabb: AABB) -> AABB {

  let mut xmin: real_t;
  let mut xmax: real_t;
  let mut ymin: real_t;
  let mut ymax: real_t;
  let mut zmin: real_t;
  let mut zmax: real_t;

  if(world_size > 1) {
    let xcells = real_floor((aabb.max(0) - aabb.min(0)) / cell_spacing) as i32;
    let ycells = real_floor((aabb.max(1) - aabb.min(1)) / cell_spacing) as i32;
    let zcells = real_floor((aabb.max(2) - aabb.min(2)) / cell_spacing) as i32;

    node_dims = get_node_config(world_size, rank, xcells, ycells, zcells);

    let xlength = (xcells / node_dims(0)) as real_t * cell_spacing;
    let ylength = (ycells / node_dims(1)) as real_t * cell_spacing;
    let zlength = (zcells / node_dims(2)) as real_t * cell_spacing;
    let rank_index = unflat_index(rank, node_dims(0), node_dims(1), node_dims(2));

    xmin = aabb.min(0) + xlength * (rank_index(0) as real_t);
    xmax = aabb.min(0) + xlength * ((rank_index(0) + 1) as real_t);
    ymin = aabb.min(1) + ylength * (rank_index(1) as real_t);
    ymax = aabb.min(1) + ylength * ((rank_index(1) + 1) as real_t);
    zmin = aabb.min(2) + zlength * (rank_index(2) as real_t);
    zmax = aabb.min(2) + zlength * ((rank_index(2) + 1) as real_t);

    if(rank_index(0) > 0) {
      xmin -= (get_sync_timesteps() as real_t) * cell_spacing;
    }

    if(rank_index(0) < node_dims(0) - 1) {
      xmax += (get_sync_timesteps() as real_t) * cell_spacing;
    } else {
      xmax += cell_spacing;
    }

    if(rank_index(1) > 0) {
      ymin -= (get_sync_timesteps() as real_t) * cell_spacing;
    }

    if(rank_index(1) < node_dims(1) - 1) {
      ymax += (get_sync_timesteps() as real_t) * cell_spacing;
    } else {
      ymax += cell_spacing;
    }

    if(rank_index(2) > 0) {
      zmin -= (get_sync_timesteps() as real_t) * cell_spacing;
    }

    if(rank_index(2) < node_dims(2) - 1) {
      zmax += (get_sync_timesteps() as real_t) * cell_spacing;
    } else {
      zmax += cell_spacing;
    }
  } else {
    node_dims = [1, 1, 1];

    xmin = aabb.min(0);
    xmax = aabb.max(0);
    ymin = aabb.min(1);
    ymax = aabb.max(1);
    zmin = aabb.min(2);
    zmax = aabb.max(2);
  }

  AABB {
    min: [xmin, ymin, zmin],
    max: [xmax, ymax, zmax]
  }
}

fn initialize_grid_comm(
  grid: Grid,
  world_size: i32,
  world_rank: i32) -> () {

  let rank_send_ptr = bitcast[&mut[i32]](rank_send_particles.data);
  let rank_recv_ptr = bitcast[&mut[i32]](rank_recv_particles.data);

  max_send_particles = 0;
  max_recv_particles = 0;

  for exchange_rank, send_begin, send_end, recv_begin, recv_end in
      communication_nodes(world_size, world_rank, grid) {

    rank_send_ptr(exchange_rank) = 0;
    rank_recv_ptr(exchange_rank) = 0;

    for cell, index in
        map_over_grid_subdomain(grid, send_begin, send_end, range, range, range) {
      rank_send_ptr(exchange_rank) += cell.size + cell.padding;
    }

    for cell, index in
        map_over_grid_subdomain(grid, recv_begin, recv_end, range, range, range) {
      rank_recv_ptr(exchange_rank) += cell.size + cell.padding;
    }

    if(max_send_particles < rank_send_ptr(exchange_rank)) {
      max_send_particles = rank_send_ptr(exchange_rank);
    }

    if(max_recv_particles < rank_recv_ptr(exchange_rank)) {
      max_recv_particles = rank_recv_ptr(exchange_rank);
    }
  }

  initialize_grid_transfer_ranges(grid);
}

fn pack_ghost_layer_cells(
  comm_buffer: Buffer,
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  begin: [i32 * 3],
  end: [i32 * 3]) -> () {

  let mut buffer_ptr = 0;

  for cell, index in
      map_over_grid_subdomain(grid, begin, end, range, range, range) {
    let flat_index = flatten_index(index, grid);
    let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
    let offset = cell_offsets(flat_index);
    let nparticles = cell.size;

    if(nparticles > 0) {
      buffer_ptr += copy_struct_of_arrays_to_buffer(
        accelerator_grid.positions_cpu, offset,
        comm_buffer, buffer_ptr, nparticles);

      buffer_ptr += copy_struct_of_arrays_to_buffer(
        accelerator_grid.velocities_cpu, offset,
        comm_buffer, buffer_ptr, nparticles);

      buffer_ptr += copy_struct_of_arrays_to_buffer(
        accelerator_grid.forces_cpu, offset,
        comm_buffer, buffer_ptr, nparticles);
    }
  }
}

fn unpack_ghost_layer_cells(
  comm_buffer: Buffer,
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  begin: [i32 * 3],
  end: [i32 * 3]) -> () {

  let mut buffer_ptr = 0;

  for cell, index in
      map_over_grid_subdomain(grid, begin, end, range, range, range) {
    let flat_index = flatten_index(index, grid);
    let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
    let offset = cell_offsets(flat_index);
    let nparticles = cell.size;

    if(nparticles > 0) {
      buffer_ptr += copy_buffer_to_struct_of_arrays(
        comm_buffer, buffer_ptr,
        accelerator_grid.positions_cpu, offset, nparticles);

      buffer_ptr += copy_buffer_to_struct_of_arrays(
        comm_buffer, buffer_ptr,
        accelerator_grid.velocities_cpu, offset, nparticles);

      buffer_ptr += copy_buffer_to_struct_of_arrays(
        comm_buffer, buffer_ptr,
        accelerator_grid.forces_cpu, offset, nparticles);
    }
  }
}

fn copy_struct_of_arrays_to_buffer(
  source: StructOfArrays3D,
  offset_source: i32,
  dest: Buffer,
  offset_dest: i32,
  size: i32) -> i32 {

  let offset_ptr = offset_dest * sizeof[real_t]();
  let nbytes = size * sizeof[real_t]();

  copy_offset(
    source.x, offset_source * sizeof[real_t](),
    dest, offset_ptr, nbytes);

  copy_offset(
    source.y, offset_source * sizeof[real_t](),
    dest, offset_ptr + nbytes, nbytes);

  copy_offset(
    source.z, offset_source * sizeof[real_t](),
    dest, offset_ptr + nbytes * 2, nbytes);

  size * 3
}

fn copy_buffer_to_struct_of_arrays(
  source: Buffer,
  offset_source: i32,
  dest: StructOfArrays3D,
  offset_dest: i32,
  size: i32) -> i32 {

  let offset_ptr = offset_source * sizeof[real_t]();
  let nbytes = size * sizeof[real_t]();

  copy_offset(
    source, offset_ptr,
    dest.x, offset_dest * sizeof[real_t](), nbytes);

  copy_offset(
    source, offset_ptr + nbytes,
    dest.y, offset_dest * sizeof[real_t](), nbytes);

  copy_offset(
    source, offset_ptr + nbytes * 2,
    dest.z, offset_dest * sizeof[real_t](), nbytes);

  size * 3
}

fn synchronize_ghost_layer_cells(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mut request: MPI_Request;
  let mut status: MPIStatus;

  let rank_send_ptr = bitcast[&mut[i32]](rank_send_particles.data);
  let rank_recv_ptr = bitcast[&mut[i32]](rank_recv_particles.data);

  let total_number_of_particles =
    accelerator_grid.total_number_of_clusters *
    accelerator_grid.cluster_size;

  resize_comm_buffers(max_send_particles * 3 * 3, max_recv_particles * 3 * 3);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.positions_accelerator,
    accelerator_grid.positions_cpu,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.velocities_accelerator,
    accelerator_grid.velocities_cpu,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.forces_accelerator,
    accelerator_grid.forces_cpu,
    total_number_of_particles);

  for exchange_rank, send_begin, send_end, recv_begin, recv_end in
      communication_nodes(world_size, world_rank, *grid) {

    if(rank_send_ptr(exchange_rank) > 0) {
      pack_ghost_layer_cells(
        comm_send_buffer, grid, accelerator_grid, send_begin, send_end);
    }

    if(rank_recv_ptr(exchange_rank) > 0) {
      mpih.irecv(
        bitcast[&mut[real_t]](comm_recv_buffer.data) as MPI_MutBuf,
        rank_recv_ptr(exchange_rank) * 3 * 3,
        mpih.double_t, exchange_rank, 0, mpih.comms.world, &mut request);
    }

    if(rank_send_ptr(exchange_rank) > 0) {
      mpih.send(
        bitcast[&mut[real_t]](comm_send_buffer.data) as MPI_MutBuf,
        rank_send_ptr(exchange_rank) * 3 * 3,
        mpih.double_t, exchange_rank, 0, mpih.comms.world);
    }

    if(rank_recv_ptr(exchange_rank) > 0) {
      mpih.wait(&mut request, &mut status);

      unpack_ghost_layer_cells(
        comm_recv_buffer, grid, accelerator_grid, recv_begin, recv_end);
    }
  }

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.positions_cpu,
    accelerator_grid.positions_accelerator,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.velocities_cpu,
    accelerator_grid.velocities_accelerator,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.forces_cpu,
    accelerator_grid.forces_accelerator,
    total_number_of_particles);
}

fn pack_ghost_layer_particles(
  comm_buffer: Buffer,
  grid: &mut Grid,
  begin: [i32 * 3],
  end: [i32 * 3],
  start_iteration: i32,
  max_particles: i32,
  packed_iterations: &mut i32,
  rmng_particles: &mut i32) -> i32 {

  let buffer_data = get_array_of_reals(comm_buffer);
  let mut buffer_ptr = 0;
  let mut iterations = 0;
  let mut packed_particles = 0;
  let mut keep_packing = true;

  *rmng_particles = 0;
  *packed_iterations = 0;

  buffer_ptr += 2;

  for cell, index in
      map_over_grid_subdomain(grid, begin, end, range, range, range) {

    let nparticles = cell.size + cell.padding;

    if(iterations >= start_iteration) {
      if(!keep_packing || packed_particles + nparticles > max_particles) {
        *rmng_particles += nparticles;
        keep_packing = false;
      } else {
        buffer_data(buffer_ptr) = cell.size as real_t;
        buffer_data(buffer_ptr + 1) = cell.padding as real_t;
        buffer_data(buffer_ptr + 2) = cell.cluster_size as real_t;

        buffer_ptr += 3;

        if(nparticles > 0) {
          copy_offset(
            cell.masses, 0,
            comm_buffer, buffer_ptr * sizeof[real_t](), nparticles);

          buffer_ptr += nparticles;

          buffer_ptr += copy_struct_of_arrays_to_buffer(
            cell.positions, 0,
            comm_buffer, buffer_ptr, nparticles);

          buffer_ptr += copy_struct_of_arrays_to_buffer(
            cell.velocities, 0,
            comm_buffer, buffer_ptr, nparticles);

          buffer_ptr += copy_struct_of_arrays_to_buffer(
            cell.forces, 0,
            comm_buffer, buffer_ptr, nparticles);

          packed_particles += nparticles;
        }

        *packed_iterations = *packed_iterations + 1;
      }
    }

    iterations++;
  }

  buffer_data(0) = *rmng_particles as real_t;
  buffer_data(1) = *packed_iterations as real_t;

  packed_particles
}

fn unpack_ghost_layer_particles(
  comm_buffer: Buffer,
  grid: &mut Grid,
  begin: [i32 * 3],
  end: [i32 * 3],
  start_iter: i32,
  unpacked_iterations: &mut i32,
  rmng_particles: &mut i32) -> i32 {

  let buffer_data = get_array_of_reals(comm_buffer);
  let mut buffer_ptr = 0;
  let mut iterations = 0;
  let mut unpacked_particles = 0;

  *rmng_particles = buffer_data(0) as i32;
  *unpacked_iterations = buffer_data(1) as i32;
  buffer_ptr += 2;

  for cell, index in
      map_over_grid_subdomain(grid, begin, end, range, range, range) {

    if(iterations >= start_iter && iterations < start_iter + *unpacked_iterations) {
      let old_nparticles = cell.size + cell.padding;

      cell.size = buffer_data(buffer_ptr) as i32;
      cell.padding = buffer_data(buffer_ptr + 1) as i32;
      cell.cluster_size = buffer_data(buffer_ptr + 2) as i32;

      buffer_ptr += 3;

      if(cell.size >= cell.capacity) {
        reallocate_cell(cell.size, cell, alloc_unaligned_cpu);
      }

      let nparticles = cell.size + cell.padding;

      grid.nparticles += nparticles - old_nparticles;

      if(nparticles > 0) {
        copy_offset(
          comm_buffer, buffer_ptr * sizeof[real_t](),
          cell.masses, 0, nparticles);

        buffer_ptr += nparticles;

        buffer_ptr += copy_buffer_to_struct_of_arrays(
          comm_buffer, buffer_ptr,
          cell.positions, 0, nparticles);

        buffer_ptr += copy_buffer_to_struct_of_arrays(
          comm_buffer, buffer_ptr,
          cell.velocities, 0, nparticles);

        buffer_ptr += copy_buffer_to_struct_of_arrays(
          comm_buffer, buffer_ptr,
          cell.forces, 0, nparticles);

        unpacked_particles += nparticles;
      }
    }

    iterations++;
  }

  unpacked_particles
}

fn exchange_ghost_layer_particles(
  grid: &mut Grid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mut request: MPI_Request;
  let mut status: MPIStatus;

  let rank_send_ptr = bitcast[&mut[i32]](rank_send_particles.data);
  let rank_recv_ptr = bitcast[&mut[i32]](rank_recv_particles.data);

  let send_particles = max_send_particles + max_send_particles / 2;
  let recv_particles = max_recv_particles + max_recv_particles / 2;
  let send_length = send_particles + send_particles * 3 * 3;
  let recv_length = recv_particles + recv_particles * 3 * 3;

  max_send_particles = 0;
  max_recv_particles = 0;

  for exchange_rank, send_begin, send_end, recv_begin, recv_end in
      communication_nodes(world_size, world_rank, *grid) {

    let mut rmng_send_ptcs: i32;
    let mut rmng_recv_ptcs: i32;
    let mut packed_iterations: i32;
    let mut unpacked_iterations: i32;

    let dims_length = 3 * (send_end(0) - send_begin(0)) *
                          (send_end(1) - send_begin(1)) *
                          (send_end(2) - send_begin(2));

    let rank_send_ptcs =
      rank_send_ptr(exchange_rank) + rank_send_ptr(exchange_rank) / 2;
    let rank_recv_ptcs =
      rank_recv_ptr(exchange_rank) + rank_recv_ptr(exchange_rank) / 2;

    resize_comm_buffers(dims_length + send_length, dims_length + recv_length);

    rank_send_ptr(exchange_rank) = pack_ghost_layer_particles(
      comm_send_buffer, grid, send_begin, send_end,
      0, rank_send_ptcs, &mut packed_iterations, &mut rmng_send_ptcs);

    mpih.irecv(
      bitcast[&mut[real_t]](comm_recv_buffer.data) as MPI_MutBuf,
      dims_length + rank_recv_ptcs + rank_recv_ptcs * 3 * 3,
      mpih.double_t, exchange_rank, 0, mpih.comms.world, &mut request);

    mpih.send(
      bitcast[&mut[real_t]](comm_send_buffer.data) as MPI_MutBuf,
      dims_length + rank_send_ptcs + rank_send_ptcs * 3 * 3,
      mpih.double_t, exchange_rank, 0, mpih.comms.world);

    mpih.wait(&mut request, &mut status);

    rank_recv_ptr(exchange_rank) = unpack_ghost_layer_particles(
      comm_recv_buffer, grid, recv_begin, recv_end,
      0, &mut unpacked_iterations, &mut rmng_recv_ptcs);

    if(rmng_recv_ptcs > 0) {
      let rmng_recv_length = rmng_recv_ptcs + rmng_recv_ptcs * 3 * 3;

      resize_comm_buffers(0, dims_length + rmng_recv_length);

      mpih.irecv(
        bitcast[&mut[real_t]](comm_recv_buffer.data) as MPI_MutBuf,
        dims_length + rmng_recv_length,
        mpih.double_t, exchange_rank, 0, mpih.comms.world, &mut request);
    }

    if(rmng_send_ptcs > 0) {
      let rmng_send_length = rmng_send_ptcs + rmng_send_ptcs * 3 * 3;

      resize_comm_buffers(dims_length + rmng_send_length, 0);

      rank_send_ptr(exchange_rank) += pack_ghost_layer_particles(
        comm_send_buffer, grid, send_begin, send_end,
        packed_iterations, rmng_send_ptcs,
        &mut packed_iterations, &mut rmng_send_ptcs);

      mpih.send(
        bitcast[&mut[real_t]](comm_send_buffer.data) as MPI_MutBuf,
        dims_length + rmng_send_length,
        mpih.double_t, exchange_rank, 0, mpih.comms.world);
    }

    if(rmng_recv_ptcs > 0) {
      mpih.wait(&mut request, &mut status);

      rank_recv_ptr(exchange_rank) += unpack_ghost_layer_particles(
        comm_recv_buffer, grid, recv_begin, recv_end,
        unpacked_iterations, &mut unpacked_iterations, &mut rmng_recv_ptcs);
    }

    if(max_send_particles < rank_send_ptr(exchange_rank)) {
      max_send_particles = rank_send_ptr(exchange_rank);
    }

    if(max_recv_particles < rank_recv_ptr(exchange_rank)) {
      max_recv_particles = rank_recv_ptr(exchange_rank);
    }
  }
}
