// Communication offsets (for GPU gather and scatter kernels)
struct CommOffsets {
    // Host send data
    send_rank_offsets: Buffer,
    send_starts: Buffer,
    send_offsets: Buffer,
    send_capacity: i32,
    send_noffsets: i32,

    // Host receive data
    recv_rank_offsets: Buffer,
    recv_starts: Buffer,
    recv_offsets: Buffer,
    recv_capacity: i32,
    recv_noffsets: i32,

    // Accelerator send data
    send_buffer_accelerator: Buffer,
    send_rank_offsets_accelerator: Buffer,
    send_starts_accelerator: Buffer,
    send_offsets_accelerator: Buffer,

    // Accelerator receive data
    recv_buffer_accelerator: Buffer,
    recv_rank_offsets_accelerator: Buffer,
    recv_starts_accelerator: Buffer,
    recv_offsets_accelerator: Buffer,

    // Number of neighbor ranks
    neighs: i32
};

// Number of nodes in each dimension
static mut gx: i32;
static mut gy: i32;
static mut gz: i32;

// Lookup table for ghost layer ranges
static mut grid_transfer_ranges: [[i32 * 4] * 9];

// Number of send and receive particles per rank
static mut rank_send_particles: Buffer;
static mut rank_recv_particles: Buffer;

// Maximum number of send and receive particles
static mut max_send_particles: i32;
static mut max_recv_particles: i32;

// Buffers for send and receive communications
static mut comm_send_buffer: Buffer;
static mut comm_recv_buffer: Buffer;

// Sizes for communication buffers
static mut comm_send_buffer_size: i32;
static mut comm_recv_buffer_size: i32;

// Timesteps to perform cell synchronization (ghost layer width)
fn @get_sync_timesteps() -> i32 { 1 }

// Must synchronize diagonals (true/false)
fn @synchronize_diagonals() -> bool { false }

// Flatten index for ranks (3D cartesian mapping)
fn @flat_index(index: [i32 * 3], nx: i32, ny: i32) -> i32 { (index(2) * ny + index(1)) * nx + index(0) }

// Unflatten index for ranks (3D cartesian mapping)
fn @unflat_index(index: i32, nx: i32, ny: i32, nz: i32, rx: &mut i32, ry: &mut i32, rz: &mut i32) -> () {
    *rx = index % nx;
    *ry = (index / nx) % ny;
    *rz = index / (nx * ny);
}

// MPI barrier
fn barrier() -> () {
    let mpih = mpi();
    let mut request: MPI_Request;

    mpih.barrier(mpih.comms.world, &mut request);
}

// Print string with rank
fn print_string_with_rank(string: &[u8]) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(string);
    print_string("\n");
    print_flush();
}

// Print i32 value with rank
fn print_i32_with_rank(field: &[u8], value: i32) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_i32(value);
    print_string("\n");
    print_flush();
}

// Print real value with rank
fn print_real_with_rank(field: &[u8], value: real_t) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_f64(value);
    print_string("\n");
    print_flush();
}

// Print real buffer with rank
fn print_real_buffer_with_rank(field: &[u8], buffer: Buffer, offset: i32, length: i32) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");

    range(0, length, |i| {
        print_f64(get_real(i + offset, buffer));
        print_string(", ");
    });

    print_string("\n");
}

// Print i32 buffer with rank
fn print_i32_buffer_with_rank(field: &[u8], buffer: Buffer, offset: i32, length: i32) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");

    range(0, length, |i| {
        print_i32(get_i32(i + offset, buffer));
        print_string(", ");
    });

    print_string("\n");
}

// Print [i32 * 3] value with rank
fn print_i32_vector_with_rank(field: &[u8], value: [i32 * 3]) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_i32(value(0));
    print_string(", ");
    print_i32(value(1));
    print_string(", ");
    print_i32(value(2));
    print_string("\n");
    print_flush();
}

// Print Vector3D value with rank
fn print_real_vector_with_rank(field: &[u8], value: Vector3D) -> () {
    let mpih = mpi();
    let mut rank: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);

    print_i32(rank);
    print_string("> ");
    print_string(field);
    print_string(": ");
    print_f64(value.x);
    print_string(", ");
    print_f64(value.y);
    print_string(", ");
    print_f64(value.z);
    print_string("\n");
    print_flush();
}

// Mirrored communication (avoid deadlocks)
fn @mirror_comm(
    xoffset: i32,
    yoffset: i32,
    zoffset: i32,
    body: fn(i32, i32, i32) -> ()) -> () {

    @@body(xoffset, yoffset, zoffset);
    @@body(-xoffset, -yoffset, -zoffset);
}

// Initialize MPI and defined data structures for communication
fn mpi_initialize(world_size: &mut i32, world_rank: &mut i32) -> () {
    let mpih = mpi();

    mpih.init();

    comm_send_buffer_size = 0;
    comm_recv_buffer_size = 0;

    mpih.comm_size(mpih.comms.world, world_size);
    mpih.comm_rank(mpih.comms.world, world_rank);

    add_mpi_allocation(sizeof[i32]() * *world_size * 2);

    rank_send_particles = alloc_unaligned_cpu(sizeof[i32]() * *world_size);
    rank_recv_particles = alloc_unaligned_cpu(sizeof[i32]() * *world_size);
}

// Finalize MPI and free data structures for communication
fn mpi_finalize() -> () {
    let mpih = mpi();

    if comm_send_buffer_size > 0 {
        release_host(comm_send_buffer);
    }

    if comm_recv_buffer_size > 0 {
        release_host(comm_recv_buffer);
    }

    release_host(rank_send_particles);
    release_host(rank_recv_particles);
    release_comm_offsets(comm_offsets_);

    mpih.finalize();
}

// Resize communication buffers (data is not preserved)
fn resize_comm_buffers(send_size: i32, recv_size: i32) -> () {
    // If new send buffer size is higher than current one
    if send_size > comm_send_buffer_size {
        if comm_send_buffer_size > 0 {
            release(comm_send_buffer);
        }

        // Allocate new buffer with new size and update size
        comm_send_buffer = alloc_unaligned_cpu(sizeof[real_t]() * send_size);
        comm_send_buffer_size = send_size;
    }

    // If new receive buffer size is higher than current one
    if recv_size > comm_recv_buffer_size {
        if comm_recv_buffer_size > 0 {
            release(comm_recv_buffer);
        }

        // Allocate new buffer with new size and update size
        comm_recv_buffer = alloc_unaligned_cpu(sizeof[real_t]() * recv_size);
        comm_recv_buffer_size = recv_size;
    }
}

// Communication pattern for 1D domain partitioning
fn communication_nodes_1d(
    world_size: i32,
    rank: i32,
    grid: &Grid,
    body: fn(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> ()) -> () {

    // If rank is not the last one
    if rank < world_size - 1 {
        // Communicate with next rank
        @@body(
            rank + 1,
            0, 0, grid.nz - get_sync_timesteps() * 2,
            grid.nx, grid.ny, grid.nz - get_sync_timesteps(),
            0, 0, grid.nz - get_sync_timesteps(),
            grid.nx, grid.ny, grid.nz
        );
    }

    // If rank is not the first one
    if rank > 0 {
        // Communicate with previous rank
        @@body(
            rank - 1,
            0, 0, get_sync_timesteps(),
            grid.nx, grid.ny, get_sync_timesteps() * 2,
            0, 0, 0,
            grid.nx, grid.ny, get_sync_timesteps()
        );
    }
}

// Get current node bounding box for 1D domain partitioning
fn @get_node_bounding_box_1d(
    world_size: i32,
    rank: i32,
    cell_spacing: real_t,
    grid_aabb: AABB) -> AABB {

    let mut zmin: real_t;
    let mut zmax: real_t;

    if world_size > 1 {
        // Number of cells in z axis
        let zcells = real_floor((grid_aabb.zmax - grid_aabb.zmin) / cell_spacing) as i32;

        // Total length for cells in z axis
        let zlength = (zcells / world_size) as real_t * cell_spacing;

        // Calculate zmin and zmax based on rank value
        zmin = grid_aabb.zmin + zlength * (rank as real_t);
        zmax = grid_aabb.zmin + zlength * ((rank + 1) as real_t);

        // If this is not the first node, include ghost layer for lower z ranks
        if rank > 0 {
            zmin -= (get_sync_timesteps() as real_t) * cell_spacing;
        }

        // If this is not the last node, include ghost layer for upper z ranks
        if rank < world_size - 1 {
            zmax += (get_sync_timesteps() as real_t) * cell_spacing;
        // Otherwise, just increase cell spacing to maximum delimiter to
        // reach any possible missing cells in the original bounding box
        } else {
            zmax += cell_spacing;
        }
    // Otherwise, just use original bounding box
    } else {
        zmin = grid_aabb.zmin;
        zmax = grid_aabb.zmax;
    }

    AABB {
        xmin: grid_aabb.xmin,
        xmax: grid_aabb.xmax,
        ymin: grid_aabb.ymin,
        ymax: grid_aabb.ymax,
        zmin: grid_aabb.zmin,
        zmax: grid_aabb.zmax
    }
}

// Get send and receive transfer ranges for communications
fn transfer_ranges(offset: i32, ncells: i32, body: fn(i32, i32, i32, i32) -> ()) -> () {
    let mut send_begin: i32;
    let mut send_end: i32;
    let mut recv_begin: i32;
    let mut recv_end: i32;

    // If offset is zero (same position), communication must comprise all
    // cells in this axis (adjacent nodes)
    if offset == 0 {
        send_begin = 0;
        send_end = ncells;
        recv_begin = 0;
        recv_end = ncells;
    // If offset is less than zero (lower position), communication must
    // comprise only last cells in this axis
    } else if offset < 0 {
        send_begin = ncells - get_sync_timesteps() * 2;
        send_end = ncells - get_sync_timesteps();
        recv_begin = ncells - get_sync_timesteps();
        recv_end = ncells;
    // If offset is greater than zero (lower position), communication must
    // comprise only first cells in this axis
    } else {
        send_begin = get_sync_timesteps();
        send_end = get_sync_timesteps() * 2;
        recv_begin = 0;
        recv_end = get_sync_timesteps();
    }

    @@body(send_begin, send_end, recv_begin, recv_end)
}

// Build lookup table for transferring ranges in all dimensions
fn initialize_grid_transfer_ranges(grid: &Grid) -> () {
    // Go through all offsets (-1, 0, +1)
    for i in range(-1, 2) {
        transfer_ranges(i, grid.nx, |send_begin, send_end, recv_begin, recv_end| {
            grid_transfer_ranges(i + 1)(0) = send_begin;
            grid_transfer_ranges(i + 1)(1) = send_end;
            grid_transfer_ranges(i + 1)(2) = recv_begin;
            grid_transfer_ranges(i + 1)(3) = recv_end;
        });

        transfer_ranges(i, grid.ny, |send_begin, send_end, recv_begin, recv_end| {
            grid_transfer_ranges(i + 4)(0) = send_begin;
            grid_transfer_ranges(i + 4)(1) = send_end;
            grid_transfer_ranges(i + 4)(2) = recv_begin;
            grid_transfer_ranges(i + 4)(3) = recv_end;
        });

        transfer_ranges(i, grid.nz, |send_begin, send_end, recv_begin, recv_end| {
            grid_transfer_ranges(i + 7)(0) = send_begin;
            grid_transfer_ranges(i + 7)(1) = send_end;
            grid_transfer_ranges(i + 7)(2) = recv_begin;
            grid_transfer_ranges(i + 7)(3) = recv_end;
        });
    }
}

// Generic communication pattern (including diagonals)
fn communication_nodes_generic(
    world_size: i32,
    rank: i32,
    grid: &Grid,
    body: fn(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> ()) -> () {

    let mut rx: i32;
    let mut ry: i32;
    let mut rz: i32;

    // Index of rank in the 3D cartesian grid
    unflat_index(rank, gx, gy, gz, &mut rx, &mut ry, &mut rz);

    // Has previous node in x axis?
    let xprev = rx > 0;
    // Has successor node in x axis?
    let xnext = rx < gx - 1;
    // Has previous node in y axis?
    let yprev = ry > 0;
    // Has successor node in y axis?
    let ynext = ry < gy - 1;
    // Has previous node in z axis?
    let zprev = rz > 0;
    // Has successor node in z axis?
    let znext = rz < gz - 1;

    // Go through possible offsets
    for xoffset in range(-1, 1) {
        for yoffset in range(-1, 1 - xoffset) {
            for zoffset in range(-1, 2) {
                // Number of zero offsets (diagonals include zero or one)
                let nzeros = (xoffset == 0) as i32 +
                             (yoffset == 0) as i32 +
                             (zoffset == 0) as i32;

                //if(nzeros == 2 || (synchronize_diagonals() && nzeros < 3)) {

                // If number of zeros is two, not a diagonal (adjacent node)
                if nzeros == 2 {
                    // Perform for original and mirrored offsets to avoid deadlocks
                    for xoff, yoff, zoff in mirror_comm(xoffset, yoffset, zoffset) {
                      // Check if node for communication is valid
                      if(
                          ((xoff < 0 && xprev) || (xoff > 0 && xnext) || (xoff == 0)) &&
                          ((yoff < 0 && yprev) || (yoff > 0 && ynext) || (yoff == 0)) &&
                          ((zoff < 0 && zprev) || (zoff > 0 && znext) || (zoff == 0))
                      ) {
                        // Exchange rank based on offsets
                        let exchange_rank = flat_index(
                            [rx + xoff, ry + yoff, rz + zoff], gx, gy
                        );

                        // Get x, y and z ranges on lookup table
                        let xranges = grid_transfer_ranges(xoff + 1);
                        let yranges = grid_transfer_ranges(yoff + 4);
                        let zranges = grid_transfer_ranges(zoff + 7);

                        // Build ranges based on vectors obtained from the lookup table
                        let send_begin = [xranges(0), yranges(0), zranges(0)];
                        let send_end   = [xranges(1), yranges(1), zranges(1)];
                        let recv_begin = [xranges(2), yranges(2), zranges(2)];
                        let recv_end   = [xranges(3), yranges(3), zranges(3)];

                        // Performs communication with rank using the obtained ranges
                        @@body( exchange_rank,
                                xranges(0), yranges(0), zranges(0),
                                xranges(1), yranges(1), zranges(1),
                                xranges(2), yranges(2), zranges(2),
                                xranges(3), yranges(3), zranges(3)  );
                      }
                  }
              }
          }
      }
    }
}

// Is this a local cell?
fn is_local_cell(cx: i32, cy: i32, cz: i32, grid: &Grid) -> bool {
    let mpih = mpi();
    let mut rank: i32;
    let mut rx: i32;
    let mut ry: i32;
    let mut rz: i32;

    let mut local_cond = true;

    mpih.comm_rank(mpih.comms.world, &mut rank);
    unflat_index(rank, gx, gy, gz, &mut rx, &mut ry, &mut rz);

    local_cond =               (rx == 0      || cx > get_sync_timesteps() - 1);
    local_cond = local_cond && (rx == gx - 1 || cx < grid.nx - get_sync_timesteps());
    local_cond = local_cond && (ry == 0      || cy > get_sync_timesteps() - 1);
    local_cond = local_cond && (ry == gy - 1 || cy < grid.ny - get_sync_timesteps());
    local_cond = local_cond && (rz == 0      || cz > get_sync_timesteps() - 1);
    local_cond = local_cond && (rz == gz - 1 || cz < grid.nz - get_sync_timesteps());

    local_cond
}

// Adjust cell index to closest local one
fn closest_local_cell(cx: i32, cy: i32, cz: i32, grid: &Grid) -> [i32 * 3] {
    let mpih = mpi();
    let mut rank: i32;
    let mut rx: i32;
    let mut ry: i32;
    let mut rz: i32;
    let mut resx = cx;
    let mut resy = cy;
    let mut resz = cz;

    mpih.comm_rank(mpih.comms.world, &mut rank);
    unflat_index(rank, gx, gy, gz, &mut rx, &mut ry, &mut rz);

    resx = select(rx > 0      && resx < get_sync_timesteps(),            get_sync_timesteps(), resx);
    resx = select(rx < gx - 1 && resx >= grid.nx - get_sync_timesteps(), grid.nx - get_sync_timesteps() - 1, resx);
    resy = select(ry > 0      && resy < get_sync_timesteps(),            get_sync_timesteps(), resy);
    resy = select(ry < gy - 1 && resy >= grid.ny - get_sync_timesteps(), grid.ny - get_sync_timesteps() - 1, resy);
    resz = select(rz > 0      && resz < get_sync_timesteps(),            get_sync_timesteps(), resz);
    resz = select(rz < gz - 1 && resz >= grid.nz - get_sync_timesteps(), grid.nz - get_sync_timesteps() - 1, resz);

    [resx, resy, resz]
}

// Check if cells are inside the same ghost layer (rank region)
fn inside_same_ghost_layer(c1x: i32, c1y: i32, c1z: i32, c2x: i32, c2y: i32, c2z: i32, grid: &Grid) -> bool {
    let mut equal_cond = false;

    equal_cond =               (c1x < get_sync_timesteps()            && c2x < get_sync_timesteps());
    equal_cond = equal_cond || (c1x >= grid.nx - get_sync_timesteps() && c2x >= grid.nx - get_sync_timesteps());
    equal_cond = equal_cond || (c1y < get_sync_timesteps()            && c2y < get_sync_timesteps());
    equal_cond = equal_cond || (c1y >= grid.ny - get_sync_timesteps() && c2y >= grid.ny - get_sync_timesteps());
    equal_cond = equal_cond || (c1z < get_sync_timesteps()            && c2z < get_sync_timesteps());
    equal_cond = equal_cond || (c1z >= grid.nz - get_sync_timesteps() && c2z >= grid.nz - get_sync_timesteps());

    equal_cond
}

// Is this the last node in the dimension?
fn last_dimension_node(@dim: i32) -> bool {
    let mpih = mpi();
    let mut rank: i32;
    let mut rx: i32;
    let mut ry: i32;
    let mut rz: i32;

    mpih.comm_rank(mpih.comms.world, &mut rank);
    unflat_index(rank, gx, gy, gz, &mut rx, &mut ry, &mut rz);

    if dim == 0 {
        rx == gx - 1
    } else if dim == 1 {
        ry == gy - 1
    } else if dim == 2 {
        rz == gz - 1
    } else {
        false
    }
}

// Default communication pattern (do not include diagonals)
fn communication_nodes(
    world_size: i32,
    rank: i32,
    grid: &Grid,
    body: fn(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> ()) -> () {

    let mut rx: i32;
    let mut ry: i32;
    let mut rz: i32;

    // Index of rank in the 3D cartesian grid
    unflat_index(rank, gx, gy, gz, &mut rx, &mut ry, &mut rz);

    // Has previous node in x axis?
    let xprev = rx > 0;
    // Has successor node in x axis?
    let xnext = rx < gx - 1;
    // Has previous node in y axis?
    let yprev = ry > 0;
    // Has successor node in y axis?
    let ynext = ry < gy - 1;
    // Has previous node in z axis?
    let zprev = rz > 0;
    // Has successor node in z axis?
    let znext = rz < gz - 1;

    if xnext {
        @@body(
            flat_index([rx + 1, ry, rz], gx, gy),
            grid.nx - get_sync_timesteps() * 2, 0, 0,
            grid.nx - get_sync_timesteps(), grid.ny, grid.nz,
            grid.nx - get_sync_timesteps(), 0, 0,
            grid.nx, grid.ny, grid.nz
        );
    }

    if xprev {
        @@body(
            flat_index([rx - 1, ry, rz], gx, gy),
            get_sync_timesteps(), 0, 0,
            get_sync_timesteps() * 2, grid.ny, grid.nz,
            0, 0, 0,
            get_sync_timesteps(), grid.ny, grid.nz
        );
    }

    if ynext {
        @@body(
            flat_index([rx, ry + 1, rz], gx, gy),
            0, grid.ny - get_sync_timesteps() * 2, 0,
            grid.nx, grid.ny - get_sync_timesteps(), grid.nz,
            0, grid.ny - get_sync_timesteps(), 0,
            grid.nx, grid.ny, grid.nz
        );
    }

    if yprev {
        @@body(
            flat_index([rx, ry - 1, rz], gx, gy),
            0, get_sync_timesteps(), 0,
            grid.nx, get_sync_timesteps() * 2, grid.nz,
            0, 0, 0,
            grid.nx, get_sync_timesteps(), grid.nz
        );
    }

    if znext {
        @@body(
            flat_index([rx, ry, rz + 1], gx, gy),
            0, 0, grid.nz - get_sync_timesteps() * 2,
            grid.nx, grid.ny, grid.nz - get_sync_timesteps(),
            0, 0, grid.nz - get_sync_timesteps(),
            grid.nx, grid.ny, grid.nz
        );
    }

    if zprev {
        @@body(
            flat_index([rx, ry, rz - 1], gx, gy),
            0, 0, get_sync_timesteps(),
            grid.nx, grid.ny, get_sync_timesteps() * 2,
            0, 0, 0,
            grid.nx, grid.ny, get_sync_timesteps()
        );
    }
}

// Get configuration for nodes according to world size and number of
// cells in each dimension
fn get_node_config(
    world_size: i32,
    rank: i32,
    xcells: i32,
    ycells: i32,
    zcells: i32,
    destx: &mut i32,
    desty: &mut i32,
    destz: &mut i32) -> () {

    // We want to find the configuration with the minimum missing factor,
    // i.e. the configuration that requires less extension of the edge
    // nodes (extension is required when number of cells is not multiple
    // of the number of nodes in a dimension)
    //let mut min_missing_factor = xcells * ycells * zcells;

    *destx = 1;
    *desty = 1;
    *destz = 1;

    // Go through all possibilities of node configurations, update only
    // when missing factor is lower than the current one

    /*
    for i in range(1, world_size) {
        if world_size % i == 0 {
            let rem_yz = world_size / i;

            for j in range(1, rem_yz) {
                if rem_yz % j == 0 {
                    let k = rem_yz / j;
                    let missing_factor = xcells % i + ycells % j + zcells % k;

                    if min_missing_factor > missing_factor {
                        *destx = i;
                        *desty = j;
                        *destz = k;
                        min_missing_factor = missing_factor;
                    }
                }
            }
        }
    }
    */

    let areax = (xcells * ycells) as f64;
    let areay = (xcells * zcells) as f64;
    let areaz = (ycells * zcells) as f64;

    let mut bestsurf = 2.0 * (areax + areay + areaz) as f64;

    for i in range(1, world_size) {
        if world_size % i == 0 {
            let rem_yz = world_size / i;

            for j in range(1, rem_yz) {
                if rem_yz % j == 0 {
                    let k = rem_yz / j;
                    let surf = areax / i as f64 / j as f64 + areay / i as f64 / k as f64 + areaz / j as f64 / k as f64;

                    if surf < bestsurf {
                        *destx = i;
                        *desty = j;
                        *destz = k;
                        bestsurf = surf;
                    }
                }
            }
        }
    }
}

// Get bounding box for current node
fn @get_node_bounding_box(
    world_size: i32,
    rank: i32,
    cell_spacing: real_t,
    aabb: AABB) -> AABB {

    let mut xmin: real_t;
    let mut xmax: real_t;
    let mut ymin: real_t;
    let mut ymax: real_t;
    let mut zmin: real_t;
    let mut zmax: real_t;

    if world_size > 1 {
        let mut rx: i32;
        let mut ry: i32;
        let mut rz: i32;

        // Number of cells in each dimension
        let xcells = real_floor((aabb.xmax - aabb.xmin) / cell_spacing) as i32;
        let ycells = real_floor((aabb.ymax - aabb.ymin) / cell_spacing) as i32;
        let zcells = real_floor((aabb.zmax - aabb.zmin) / cell_spacing) as i32;

        // Get configuration of nodes
        get_node_config(world_size, rank, xcells, ycells, zcells, &mut gx, &mut gy, &mut gz);

        // Length of each node in each dimension
        let xlength = (xcells / gx) as real_t * cell_spacing;
        let ylength = (ycells / gy) as real_t * cell_spacing;
        let zlength = (zcells / gz) as real_t * cell_spacing;

        // 3D cartesian position of current rank
        unflat_index(rank, gx, gy, gz, &mut rx, &mut ry, &mut rz);

        // Calculate boundaries using lengths in each dimension
        xmin = aabb.xmin + xlength * (rx as real_t);
        xmax = aabb.xmin + xlength * ((rx + 1) as real_t);
        ymin = aabb.ymin + ylength * (ry as real_t);
        ymax = aabb.ymin + ylength * ((ry + 1) as real_t);
        zmin = aabb.zmin + zlength * (rz as real_t);
        zmax = aabb.zmin + zlength * ((rz + 1) as real_t);

        // In the next steps, we extend the boundary for nodes that are
        // not in the edges in each dimension, this is done to comprise
        // the ghost layer area in the bounding box, in last nodes for
        // each dimension, the domain is also extended in the size of
        // the cell spacing to adjust boxes that are not multiple of
        // the cell spacing sizes

        if rx > 0 {
            xmin -= (get_sync_timesteps() as real_t) * cell_spacing;
        }

        if rx < gx - 1 {
            xmax += (get_sync_timesteps() as real_t) * cell_spacing;
        } else {
            xmax += cell_spacing;
        }

        if ry > 0 {
            ymin -= (get_sync_timesteps() as real_t) * cell_spacing;
        }

        if ry < gy - 1 {
            ymax += (get_sync_timesteps() as real_t) * cell_spacing;
        } else {
            ymax += cell_spacing;
        }

        if rz > 0 {
            zmin -= (get_sync_timesteps() as real_t) * cell_spacing;
        }

        if rz < gz - 1 {
            zmax += (get_sync_timesteps() as real_t) * cell_spacing;
        } else {
            zmax += cell_spacing;
        }
    } else {
        gx = 1;
        gy = 1;
        gz = 1;

        xmin = aabb.xmin;
        xmax = aabb.xmax;
        ymin = aabb.ymin;
        ymax = aabb.ymax;
        zmin = aabb.zmin;
        zmax = aabb.zmax;
    }

    AABB {
        xmin: xmin,
        xmax: xmax,
        ymin: ymin,
        ymax: ymax,
        zmin: zmin,
        zmax: zmax
    }
}

// Initialize grid communication
fn initialize_comm(
    grid: &Grid,
    comm_offsets: &mut CommOffsets,
    world_size: i32,
    world_rank: i32) -> () {

    let cell_sizes = get_array_of_i32(grid.cell_sizes_cpu);
    let rank_send_ptr = bitcast[&mut[i32]](rank_send_particles.data);
    let rank_recv_ptr = bitcast[&mut[i32]](rank_recv_particles.data);

    let mut neighs = 0;

    max_send_particles = 0;
    max_recv_particles = 0;

    for exchange_rank,
        send_begin_x, send_begin_y, send_begin_z,
        send_end_x, send_end_y, send_end_z,
        recv_begin_x, recv_begin_y, recv_begin_z,
        recv_end_x, recv_end_y, recv_end_z in
        communication_nodes(world_size, world_rank, grid) {

        let mut send_cells = 0;
        let mut recv_cells = 0;

        rank_send_ptr(exchange_rank) = 0;
        rank_recv_ptr(exchange_rank) = 0;

        for flat_index, index in
            map_over_grid_subdomain(
                grid,
                send_begin_x, send_begin_y, send_begin_z,
                send_end_x, send_end_y, send_end_z,
                range, range, range) {

            let cell_size = cell_sizes(flat_index);
            rank_send_ptr(exchange_rank) += cell_size;
            send_cells++;
        }

        for flat_index, index in
            map_over_grid_subdomain(
                grid,
                recv_begin_x, recv_begin_y, recv_begin_z,
                recv_end_x, recv_end_y, recv_end_z,
                range, range, range) {

            let cell_size = cell_sizes(flat_index);
            rank_recv_ptr(exchange_rank) += 20;
            recv_cells++;
        }

        neighs++;
        max_send_particles = math.max(max_send_particles, rank_send_ptr(exchange_rank));
        max_recv_particles = math.max(max_recv_particles, rank_recv_ptr(exchange_rank));
    }

    // Allocate communication offsets for GPU data transfer
    alloc_comm_offsets(comm_offsets, world_size, neighs, max_send_particles, max_recv_particles);

    initialize_grid_transfer_ranges(grid);
}

// Synchronize ghost layer cells with neighbor ranks
fn synchronize_ghost_layer_cells(
    grid: Grid,
    comm_offsets: &CommOffsets,
    world_size: i32,
    world_rank: i32) -> () {

    let mpih = mpi();
    let mut request: MPI_Request;
    let mut status: MPIStatus;

    let rank_send_ptr = bitcast[&mut[i32]](rank_send_particles.data);
    let rank_recv_ptr = bitcast[&mut[i32]](rank_recv_particles.data);
    let (resize_send, resize_recv) = get_comm_buffer_sizes(comm_offsets.neighs, max_send_particles, max_recv_particles);

    resize_comm_buffers(resize_send, resize_recv);
    gather_ghost_layer_cells(comm_send_buffer, *comm_offsets, grid);

    for exchange_rank,
        send_begin_x, send_begin_y, send_begin_z,
        send_end_x, send_end_y, send_end_z,
        recv_begin_x, recv_begin_y, recv_begin_z,
        recv_end_x, recv_end_y, recv_end_z in
        communication_nodes(world_size, world_rank, grid) {

        let (send_start, recv_start) = get_comm_buffer_starts(exchange_rank, *comm_offsets);

        pack_ghost_layer_cells(
            comm_send_buffer, grid,
            send_begin_x, send_begin_y, send_begin_z,
            send_end_x, send_end_y, send_end_z);

        mpih.irecv(
            bitcast[&mut[real_t]](&comm_recv_buffer.data(recv_start)) as MPI_MutBuf,
            rank_recv_ptr(exchange_rank) * 3,
            mpih.double_t, exchange_rank, 0, mpih.comms.world, &mut request);

        mpih.send(
            bitcast[&mut[real_t]](&comm_send_buffer.data(send_start)) as MPI_MutBuf,
            rank_send_ptr(exchange_rank) * 3,
            mpih.double_t, exchange_rank, 0, mpih.comms.world);

        mpih.wait(&mut request, &mut status);

        unpack_ghost_layer_cells(
            comm_recv_buffer, grid,
            recv_begin_x, recv_begin_y, recv_begin_z,
            recv_end_x, recv_end_y, recv_end_z);
    }

    scatter_ghost_layer_cells(comm_recv_buffer, *comm_offsets, grid);
}

// Pack ghost layer particles (exchange communication) in the CPU
fn pack_ghost_layer_particles(
    comm_buffer: Buffer,
    grid: &mut Grid,
    begin_x: i32,
    begin_y: i32,
    begin_z: i32,
    end_x: i32,
    end_y: i32,
    end_z: i32,
    start_iteration: i32,
    max_particles: i32,
    packed_iterations: &mut i32,
    rmng_particles: &mut i32) -> i32 {

    let buffer_data = get_array_of_reals(comm_buffer);
    let cell_particles = get_array_of_i32(grid.cell_particles_cpu);
    let cell_sizes = get_array_of_i32(grid.cell_sizes_cpu);
    let mut buffer_index = 0;
    let mut iterations = 0;
    let mut packed_particles = 0;
    let mut keep_packing = true;

    *rmng_particles = 0;
    *packed_iterations = 0;

    buffer_index += 2;

    map_over_grid_subdomain(grid, begin_x, begin_y, begin_z, end_x, end_y, end_z, range, range, range, |flat_index, index| {
        let cell_size = cell_sizes(flat_index);
        let cell_offset = get_cell_offset(flat_index, grid);

        if iterations >= start_iteration {
            if !keep_packing || packed_particles + cell_size > max_particles {
                *rmng_particles += cell_size;
                keep_packing = false;
            } else {
                buffer_data(buffer_index) = cell_size as real_t;
                buffer_index++;

                range(0, cell_size, |cell_particle_index| {
                    let particle_index = cell_particles(cell_offset + cell_particle_index);

                    copy_offset(
                        grid.masses_cpu, particle_index * sizeof[real_t](),
                        comm_buffer, buffer_index * sizeof[real_t](), sizeof[real_t]());

                    buffer_index++;

                    copy_3d_arrays_to_buffer(
                        grid.positions_cpu, particle_index * sizeof[real_t](),
                        comm_buffer, buffer_index * sizeof[real_t](), sizeof[real_t]());

                    buffer_index += 3;

                    copy_3d_arrays_to_buffer(
                        grid.velocities_cpu, particle_index * sizeof[real_t](),
                        comm_buffer, buffer_index * sizeof[real_t](), sizeof[real_t]());

                    buffer_index += 3;
                });

                packed_particles += cell_size;
                (*packed_iterations)++;
            }
        }

        iterations++;
    });

    buffer_data(0) = *rmng_particles as real_t;
    buffer_data(1) = *packed_iterations as real_t;

    packed_particles
}

// Unpack ghost layer particles (exchange communication) in the CPU
fn unpack_ghost_layer_particles(
    comm_buffer: Buffer,
    grid: &mut Grid,
    begin_x: i32,
    begin_y: i32,
    begin_z: i32,
    end_x: i32,
    end_y: i32,
    end_z: i32,
    start_iter: i32,
    unpacked_iterations: &mut i32,
    rmng_particles: &mut i32) -> i32 {

    let buffer_data = get_array_of_reals(comm_buffer);
    let cell_particles = get_array_of_i32(grid.cell_particles_cpu);
    let cell_sizes = get_array_of_i32(grid.cell_sizes_cpu);
    let mut buffer_index = 0;
    let mut iterations = 0;
    let mut unpacked_particles = 0;

    *rmng_particles = buffer_data(0) as i32;
    *unpacked_iterations = buffer_data(1) as i32;
    buffer_index += 2;

    map_over_grid_subdomain(grid, begin_x, begin_y, begin_z, end_x, end_y, end_z, range, range, range, |flat_index, index| {
        if iterations >= start_iter && iterations < start_iter + *unpacked_iterations {
            let cell_size = cell_sizes(flat_index);
            let nparticles = buffer_data(buffer_index) as i32;

            buffer_index++;
            add_ghost_cell_slots(flat_index, nparticles, cell_size, grid);

            let cell_offset = get_cell_offset(flat_index, grid);

            range(cell_size, nparticles + cell_size, |cell_particle_index| {
                let particle_index = cell_particles(cell_offset + cell_particle_index);

                copy_offset(
                    comm_buffer, buffer_index * sizeof[real_t](),
                    grid.masses_cpu, particle_index * sizeof[real_t](), sizeof[real_t]());

                buffer_index += 1;

                copy_buffer_to_3d_arrays(
                    comm_buffer, buffer_index * sizeof[real_t](),
                    grid.positions_cpu, particle_index * sizeof[real_t](), sizeof[real_t]());

                buffer_index += 3;

                copy_buffer_to_3d_arrays(
                    comm_buffer, buffer_index * sizeof[real_t](),
                    grid.velocities_cpu, particle_index * sizeof[real_t](), sizeof[real_t]());

                buffer_index += 3;
            });

            unpacked_particles += nparticles;
        }

        iterations++;
    });

    unpacked_particles
}

// Exchange ghost layer particles with neighbor ranks (here the number of
// particles is also updated)
fn exchange_ghost_layer_particles(
    grid: &mut Grid,
    comm_offsets: &mut CommOffsets,
    world_size: i32,
    world_rank: i32) -> () {

    let mpih = mpi();
    let mut request: MPI_Request;
    let mut status: MPIStatus;

    let cell_sizes = get_array_of_i32(grid.cell_sizes_cpu);

    let rank_send_ptr = bitcast[&mut[i32]](rank_send_particles.data);
    let rank_recv_ptr = bitcast[&mut[i32]](rank_recv_particles.data);

    let send_particles = max_send_particles * 2;
    let recv_particles = max_recv_particles * 2;
    let send_length = send_particles + send_particles * 2 * 3;
    let recv_length = recv_particles + recv_particles * 2 * 3;

    max_send_particles = 0;
    max_recv_particles = 0;

    grid.nghost = 0;

    for exchange_rank,
        send_begin_x, send_begin_y, send_begin_z,
        send_end_x, send_end_y, send_end_z,
        recv_begin_x, recv_begin_y, recv_begin_z,
        recv_end_x, recv_end_y, recv_end_z in
        communication_nodes(world_size, world_rank, grid) {

        map_over_grid_subdomain(
            grid, recv_begin_x, recv_begin_y, recv_begin_z, recv_end_x, recv_end_y, recv_end_z, range, range, range,
            |flat_index, index| {
                cell_sizes(flat_index) = 0;
            }
        );
    }

    for exchange_rank,
        send_begin_x, send_begin_y, send_begin_z,
        send_end_x, send_end_y, send_end_z,
        recv_begin_x, recv_begin_y, recv_begin_z,
        recv_end_x, recv_end_y, recv_end_z in
        communication_nodes(world_size, world_rank, grid) {

        let mut rmng_send_ptcs: i32;
        let mut rmng_recv_ptcs: i32;
        let mut packed_iterations: i32;
        let mut unpacked_iterations: i32;

        let dims_length = (send_end_x - send_begin_x) *
                          (send_end_y - send_begin_y) *
                          (send_end_z - send_begin_z);

        // We insert some extra space to try performing just one-step communication,
        // if the buffer sizes for both the sender and receptor are not enough
        // to comprise the new particle sizes, we must then send the remaining particles
        // in another communication step (receptor just know the exactly size after
        // receiving the first message)

        let rank_send_ptcs = rank_send_ptr(exchange_rank) * 2;
        let rank_recv_ptcs = rank_recv_ptr(exchange_rank) * 2;

        resize_comm_buffers(dims_length + send_length, dims_length + recv_length);

        rank_send_ptr(exchange_rank) = pack_ghost_layer_particles(
            comm_send_buffer, grid,
            send_begin_x, send_begin_y, send_begin_z, send_end_x, send_end_y, send_end_z,
            0, rank_send_ptcs, &mut packed_iterations, &mut rmng_send_ptcs);

        mpih.irecv(
            bitcast[&mut[real_t]](comm_recv_buffer.data) as MPI_MutBuf,
            dims_length + rank_recv_ptcs + rank_recv_ptcs * 2 * 3,
            mpih.double_t, exchange_rank, 0, mpih.comms.world, &mut request);

        mpih.send(
            bitcast[&mut[real_t]](comm_send_buffer.data) as MPI_MutBuf,
            dims_length + rank_send_ptcs + rank_send_ptcs * 2 * 3,
            mpih.double_t, exchange_rank, 0, mpih.comms.world);

        mpih.wait(&mut request, &mut status);

        rank_recv_ptr(exchange_rank) = unpack_ghost_layer_particles(
            comm_recv_buffer, grid,
            recv_begin_x, recv_begin_y, recv_begin_z, recv_end_x, recv_end_y, recv_end_z,
            0, &mut unpacked_iterations, &mut rmng_recv_ptcs);

        if rmng_recv_ptcs > 0 {
            let rmng_recv_length = rmng_recv_ptcs + rmng_recv_ptcs * 2 * 3;

            resize_comm_buffers(0, dims_length + rmng_recv_length);

            mpih.irecv(
                bitcast[&mut[real_t]](comm_recv_buffer.data) as MPI_MutBuf,
                dims_length + rmng_recv_length,
                mpih.double_t, exchange_rank, 0, mpih.comms.world, &mut request);
        }

        if rmng_send_ptcs > 0 {
            let rmng_send_length = rmng_send_ptcs + rmng_send_ptcs * 2 * 3;

            resize_comm_buffers(dims_length + rmng_send_length, 0);

            rank_send_ptr(exchange_rank) += pack_ghost_layer_particles(
                comm_send_buffer, grid,
                send_begin_x, send_begin_y, send_begin_z, send_end_x, send_end_y, send_end_z,
                packed_iterations, rmng_send_ptcs,
                &mut packed_iterations, &mut rmng_send_ptcs);

            mpih.send(
                bitcast[&mut[real_t]](comm_send_buffer.data) as MPI_MutBuf,
                dims_length + rmng_send_length,
                mpih.double_t, exchange_rank, 0, mpih.comms.world);
        }

        if rmng_recv_ptcs > 0 {
            mpih.wait(&mut request, &mut status);

            rank_recv_ptr(exchange_rank) += unpack_ghost_layer_particles(
                comm_recv_buffer, grid,
                recv_begin_x, recv_begin_y, recv_begin_z, recv_end_x, recv_end_y, recv_end_z,
                unpacked_iterations, &mut unpacked_iterations, &mut rmng_recv_ptcs);
        }

        max_send_particles = math.max(max_send_particles, rank_send_ptr(exchange_rank));
        max_recv_particles = math.max(max_recv_particles, rank_recv_ptr(exchange_rank));
    }

    resize_comm_offsets(comm_offsets, world_size, max_send_particles, max_recv_particles);
}

fn get_node_particles_amount(grid: Grid, world_size: i32, world_rank: i32) -> (i32, i32) {
    (grid.nparticles, grid.nghost)
}

fn reduce_time(local_time: f64, global_time: &mut f64) -> () {
    let mpih = mpi();
    let mut local = local_time;

    mpih.allreduce(&mut local as MPI_MutBuf, global_time as MPI_MutBuf, 1, mpih.double_t, mpih.ops.max, mpih.comms.world);
}

fn reduce_i32_sum(local_value: i32, global_value: &mut i32) -> () {
    let mpih = mpi();
    let mut local = local_value;

    mpih.allreduce(&mut local as MPI_MutBuf, global_value as MPI_MutBuf, 1, mpih.int_t, mpih.ops.sum, mpih.comms.world);
}

fn reduce_i64_sum(local_value: i64, global_value: &mut i64) -> () {
    let mpih = mpi();
    let mut local = local_value;

    mpih.allreduce(&mut local as MPI_MutBuf, global_value as MPI_MutBuf, 1, mpih.int64_t, mpih.ops.sum, mpih.comms.world);
}
