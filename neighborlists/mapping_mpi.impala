fn @get_comm_time_steps() -> i32 { 1 }

fn mpi_initialize(world_size: &mut i32, world_rank: &mut i32) -> () {
  let mpih = mpi();

  mpih.init();

  mpih.comm_size(mpih.comms.world, world_size);
  mpih.comm_rank(mpih.comms.world, world_rank);
}

fn mpi_finalize() -> () {
  let mpih = mpi();

  mpih.finalize();
}

fn mpi_send_struct_of_arrays(mpih: MPI, arr : StructOfArrays3D, size : i32, dest : i32) -> () {
  mpih.send(arr.x.data as MPI_MutBuf, size, mpih.double_t, dest, 0, mpih.comms.world);
  mpih.send(arr.y.data as MPI_MutBuf, size, mpih.double_t, dest, 0, mpih.comms.world);
  mpih.send(arr.z.data as MPI_MutBuf, size, mpih.double_t, dest, 0, mpih.comms.world);
}

fn mpi_recv_struct_of_arrays(mpih: MPI, arr: StructOfArrays3D, size: i32, source: i32) -> () {
  let mut status : MPIStatus;

  mpih.recv(arr.x.data as MPI_MutBuf, size, mpih.double_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(arr.y.data as MPI_MutBuf, size, mpih.double_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(arr.z.data as MPI_MutBuf, size, mpih.double_t, source, 0, mpih.comms.world, &mut status);
}

fn mpi_send_grid_subdomain(mpih: MPI, grid: Grid, begin: [i32 * 3], end: [i32 * 3], dest: i32) -> () {
  let nxyz = [
    end(0) - begin(0),
    end(1) - begin(1),
    end(2) - begin(2)
  ];

  mpih.send(&nxyz as MPI_MutBuf, 3, mpih.int_t, dest, 0, mpih.comms.world);
  mpih.send(&grid.spacing as MPI_MutBuf, 1, mpih.double_t, dest, 0, mpih.comms.world);
  mpih.send(&grid.nparticles as MPI_MutBuf, 1, mpih.int_t, dest, 0, mpih.comms.world);
  mpih.send(&grid.aabb.min as MPI_MutBuf, 3, mpih.double_t, dest, 0, mpih.comms.world);
  mpih.send(&grid.aabb.max as MPI_MutBuf, 3, mpih.double_t, dest, 0, mpih.comms.world);

  for cell, cell_index in map_over_grid_subdomain(grid, begin, end, range, range, range) {
    mpi_send_cell(mpih, *cell, dest);
  }
}

fn mpi_recv_grid(mpih: MPI, grid: &mut Grid, source: i32, allocate: fn(i32) -> Buffer) -> () {
  let mut status : MPIStatus;
  let mut nxyz : [i32 * 3];

  mpih.recv(&nxyz as MPI_MutBuf, 3, mpih.int_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(&grid.spacing as MPI_MutBuf, 1, mpih.double_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(&grid.nparticles as MPI_MutBuf, 1, mpih.int_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(&grid.aabb.min as MPI_MutBuf, 3, mpih.double_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(&grid.aabb.max as MPI_MutBuf, 3, mpih.double_t, source, 0, mpih.comms.world, &mut status);

  grid.nx = nxyz(0);
  grid.ny = nxyz(1);
  grid.nz = nxyz(2);

  grid.orig_nx = nxyz(0);
  grid.orig_ny = nxyz(1);
  grid.orig_nz = nxyz(2);

  grid.cells = allocate(grid.nx * grid.ny * grid.nz * sizeof[Cell]());

  for cell, cell_index in map_over_grid(grid, range, range, range) {
    mpi_recv_cell(mpih, cell, flatten_index(cell_index, grid), source, true, allocate);
  }
}

fn mpi_send_grid_cells(mpih: MPI, grid: Grid, begin: [i32 * 3], end: [i32 * 3], dest: i32) -> () {
  for cell, cell_index in map_over_grid_subdomain(grid, begin, end, range, range, range) {
    mpi_send_cell(mpih, *cell, dest);
  }
}

fn mpi_recv_grid_cells(mpih: MPI, grid: &mut Grid, begin: [i32 * 3], end: [i32 * 3], source: i32, allocate: fn(i32) -> Buffer) -> () {
  for cell, cell_index in map_over_grid_subdomain(grid, begin, end, range, range, range) {
    mpi_recv_cell(mpih, cell, flatten_index(cell_index, grid), source, false, allocate);
  }
}

fn mpi_send_cell(mpih: MPI, cell: Cell, dest: i32) -> () {
  mpih.send(&cell.size as MPI_MutBuf, 1, mpih.int_t, dest, 0, mpih.comms.world);
  mpih.send(&cell.padding as MPI_MutBuf, 1, mpih.int_t, dest, 0, mpih.comms.world);
  mpih.send(&cell.capacity as MPI_MutBuf, 1, mpih.int_t, dest, 0, mpih.comms.world);
  mpih.send(&cell.cluster_size as MPI_MutBuf, 1, mpih.int_t, dest, 0, mpih.comms.world);

  mpih.send(cell.masses.data as MPI_MutBuf, cell.capacity, mpih.double_t, dest, 0, mpih.comms.world);
  mpi_send_struct_of_arrays(mpih, cell.positions, cell.capacity, dest);
  mpi_send_struct_of_arrays(mpih, cell.velocities, cell.capacity, dest);
  mpi_send_struct_of_arrays(mpih, cell.forces, cell.capacity, dest);
}

fn mpi_recv_cell(mpih: MPI, cell: &mut Cell, cell_index : i32, source: i32, must_allocate: bool, allocate: fn(i32) -> Buffer) -> () {
  let mut status : MPIStatus;
  let mut new_capacity : i32;

  mpih.recv(&cell.size as MPI_MutBuf, 1, mpih.int_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(&cell.padding as MPI_MutBuf, 1, mpih.int_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(&new_capacity as MPI_MutBuf, 1, mpih.int_t, source, 0, mpih.comms.world, &mut status);
  mpih.recv(&cell.cluster_size as MPI_MutBuf, 1, mpih.int_t, source, 0, mpih.comms.world, &mut status);

  cell.index = cell_index;

  if(must_allocate || new_capacity > cell.capacity) {
    cell.capacity = new_capacity;
    cell.masses = allocate(cell.capacity * sizeof[real_t]());
    cell.positions = allocate_struct_of_arrays(cell.capacity);
    cell.velocities = allocate_struct_of_arrays(cell.capacity);
    cell.forces = allocate_struct_of_arrays(cell.capacity);
  }

  mpih.recv(cell.masses.data as MPI_MutBuf, cell.capacity, mpih.double_t, source, 0, mpih.comms.world, &mut status);
  mpi_recv_struct_of_arrays(mpih, cell.positions, cell.capacity, source);
  mpi_recv_struct_of_arrays(mpih, cell.velocities, cell.capacity, source);
  mpi_recv_struct_of_arrays(mpih, cell.forces, cell.capacity, source);
}

fn mpi_scatter_data(grid: &mut Grid, world_size: i32, world_rank: i32) -> () {
  let mpih = mpi();

  if(world_rank == 0) {
    for i in range(1, world_size) {
      let mut dom_begin : [i32 * 3];
      let mut dom_end : [i32 * 3];

      dom_begin = [0, 0, i * grid.nz / world_size - get_comm_time_steps()];
      dom_end = [grid.nx, grid.ny, grid.nz];

      mpi_send_grid_subdomain(mpih, *grid, dom_begin, dom_end, i);
      grid.nz = (grid.nz / world_size) + get_comm_time_steps();
    }
  } else {
    mpi_recv_grid(mpih, grid, 0, alloc_cpu);
  }
}

fn mpi_gather_data(grid: &mut Grid, world_size: i32, world_rank: i32) -> () {
  let mpih = mpi();
  let mut dom_begin : [i32 * 3];
  let mut dom_end : [i32 * 3];

  if(world_rank == 0) {
    for i in range(1, world_size) {
      dom_begin = [0, 0, i * grid.nz / world_size];
      dom_end = [grid.nx, grid.ny, grid.nz];

      mpi_recv_grid_cells(mpih, grid, dom_begin, dom_end, i, alloc_cpu);
      grid.nz = grid.orig_nz;
    }
  } else {
    dom_begin = [0, 0, get_comm_time_steps()];
    dom_end = [grid.nx, grid.ny, grid.nz];

    mpi_send_grid_cells(mpih, *grid, dom_begin, dom_end, 0);
  }
}

fn mpi_synchronize_ghost_zone(grid: &mut Grid, world_size: i32, world_rank: i32) -> () {
  let mpih = mpi();

  if(world_rank % 2 == 0) {
    mpi_recv_grid_cells(
      mpih, grid,
      [0, 0, grid.nz - get_comm_time_steps()],
      [grid.nx, grid.ny, grid.nz],
      world_rank + 1, alloc_cpu);

    mpi_send_grid_cells(
      mpih, *grid,
      [0, 0, grid.nz - get_comm_time_steps() * 2],
      [grid.nx, grid.ny, grid.nz - get_comm_time_steps()],
      world_rank + 1);
  } else {
    mpi_send_grid_cells(
      mpih, *grid,
      [0, 0, get_comm_time_steps()],
      [grid.nx, grid.ny, get_comm_time_steps() * 2],
      world_rank - 1);

    mpi_recv_grid_cells(
      mpih, grid,
      [0, 0, 0],
      [grid.nx, grid.ny, get_comm_time_steps()],
      world_rank - 1, alloc_cpu);
  }
}
