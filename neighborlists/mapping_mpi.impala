static mut max_send_particles : i32;
static mut max_recv_particles : i32;
static mut rank_send_particles : [i32 * 8];
static mut rank_recv_particles : [i32 * 8];

fn @get_comm_time_steps() -> i32 { 2 }

fn mpi_initialize(world_size: &mut i32, world_rank: &mut i32) -> () {
  let mpih = mpi();

  mpih.init();

  mpih.comm_size(mpih.comms.world, world_size);
  mpih.comm_rank(mpih.comms.world, world_rank);
}

fn mpi_finalize() -> () {
  let mpih = mpi();

  mpih.finalize();
}

fn mpi_send_subdomains(
  world_size: i32,
  rank: i32,
  grid: Grid,
  body: fn(i32, [i32 * 3], [i32 * 3]) -> ()) -> () {

  if(rank > 0) {
    body(
      rank - 1,
      [0, 0, get_comm_time_steps()],
      [grid.nx, grid.ny, get_comm_time_steps() * 2]
    );
  }

  if(rank < world_size - 1) {
    body(
      rank + 1,
      [0, 0, grid.nz - get_comm_time_steps() * 2],
      [grid.nx, grid.ny, grid.nz - get_comm_time_steps()]
    );
  }
}

fn mpi_receive_subdomains(
  world_size: i32,
  rank: i32,
  grid: Grid,
  body: fn(i32, [i32 * 3], [i32 * 3]) -> ()) -> () {

  if(rank < world_size - 1) {
    body(
      rank + 1,
      [0, 0, grid.nz - get_comm_time_steps()],
      [grid.nx, grid.ny, grid.nz]
    );
  }

  if(rank > 0) {
    body(
      rank - 1,
      [0, 0, 0],
      [grid.nx, grid.ny, get_comm_time_steps()]
    );
  }
}

fn @mpi_get_rank_bounding_box(
  world_size: i32,
  rank: i32,
  cell_spacing: f64,
  grid_aabb: AABB) -> AABB {

  let mut zmin: f64;
  let mut zmax: f64;

  if(world_size > 1) {
    let zcells = math.floor(
      (grid_aabb.max(2) - grid_aabb.min(2)) / cell_spacing
    ) as i32;

    let zlength = (zcells / world_size) as f64 * cell_spacing;

    zmin = grid_aabb.min(2) + zlength * (rank as f64);
    zmax = grid_aabb.min(2) + zlength * ((rank + 1) as f64);

    if(rank > 0) {
      zmin -= (get_comm_time_steps() as f64) * cell_spacing;
    }

    if(rank < world_size - 1) {
      zmax += (get_comm_time_steps() as f64) * cell_spacing;
    } else {
      zmax += cell_spacing;
    }

    /*
    print_i32(rank);
    print_string("- zmin = ");
    print_f64(zmin);
    print_string(", zmax = ");
    print_f64(zmax);
    print_string(", cell_spacing = ");
    print_f64(cell_spacing);
    print_string("\n");
    */
  } else {
    zmin = grid_aabb.min(2);
    zmax = grid_aabb.max(2);
  }

  AABB {
    min: [grid_aabb.min(0), grid_aabb.min(1), zmin],
    max: [grid_aabb.max(0), grid_aabb.max(1), zmax]
  }
}

fn mpi_initialize_grid_comm(
  grid: Grid,
  world_size: i32,
  world_rank: i32) -> () {

  max_send_particles = 0;
  max_recv_particles = 0;

  for dest_rank, begin, end in
      mpi_send_subdomains(world_size, world_rank, grid) {

    rank_send_particles(dest_rank) = 0;

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {
      rank_send_particles(dest_rank) += cell.size + cell.padding;
    }

    if(max_send_particles < rank_send_particles(dest_rank)) {
      max_send_particles = rank_send_particles(dest_rank);
    }
  }

  for source_rank, begin, end in
      mpi_receive_subdomains(world_size, world_rank, grid) {

    rank_recv_particles(source_rank) = 0;

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {
      rank_recv_particles(source_rank) += cell.size + cell.padding;
    }

    if(max_recv_particles < rank_recv_particles(source_rank)) {
      max_recv_particles = rank_recv_particles(source_rank);
    }
  }
}

fn mpi_send_ghost_zone(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mpi_buffer = alloc_cpu(sizeof[real_t]() * (max_send_particles * 3 * 3));

  for dest_rank, begin, end in
      mpi_send_subdomains(world_size, world_rank, *grid) {

    let mut buffer_ptr = 0;

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {
      let flat_index = flatten_index(index, grid);
      let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
      let offset = cell_offsets(flat_index);
      let nparticles = cell.size + cell.padding;

      if(nparticles > 0) {
        buffer_ptr += copy_struct_of_arrays_to_buffer(
          accelerator_grid.positions_cpu, offset,
          mpi_buffer, buffer_ptr, nparticles);

        buffer_ptr += copy_struct_of_arrays_to_buffer(
          accelerator_grid.velocities_cpu, offset,
          mpi_buffer, buffer_ptr, nparticles);

        buffer_ptr += copy_struct_of_arrays_to_buffer(
          accelerator_grid.forces_cpu, offset,
          mpi_buffer, buffer_ptr, nparticles);
      }
    }

    mpih.send(
      bitcast[&mut[real_t]](mpi_buffer.data) as MPI_MutBuf,
      rank_send_particles(dest_rank) * 3 * 3,
      mpih.double_t, dest_rank, 0, mpih.comms.world);
  }

  release(mpi_buffer);
}

fn mpi_recv_ghost_zone(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mpi_buffer = alloc_cpu(sizeof[real_t]() * (max_recv_particles * 3 * 3));
  let mut status: MPIStatus;

  for source_rank, begin, end in
      mpi_receive_subdomains(world_size, world_rank, *grid) {

    let mut buffer_ptr = 0;

    mpih.recv(
      bitcast[&mut[real_t]](mpi_buffer.data) as MPI_MutBuf,
      rank_recv_particles(source_rank) * 3 * 3,
      mpih.double_t, source_rank, 0, mpih.comms.world, &mut status);

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {
      let flat_index = flatten_index(index, grid);
      let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
      let offset = cell_offsets(flat_index);
      let nparticles = cell.size + cell.padding;

      if(nparticles > 0) {
        buffer_ptr += copy_buffer_to_struct_of_arrays(
          mpi_buffer, buffer_ptr,
          accelerator_grid.positions_cpu, offset, nparticles);

        buffer_ptr += copy_buffer_to_struct_of_arrays(
          mpi_buffer, buffer_ptr,
          accelerator_grid.velocities_cpu, offset, nparticles);

        buffer_ptr += copy_buffer_to_struct_of_arrays(
          mpi_buffer, buffer_ptr,
          accelerator_grid.forces_cpu, offset, nparticles);
      }
    }
  }

  release(mpi_buffer);
}

fn copy_struct_of_arrays_to_buffer(
  source: StructOfArrays3D,
  offset_source: i32,
  dest: Buffer,
  offset_dest: i32,
  size: i32) -> i32 {

  let offset_ptr = offset_dest * sizeof[real_t]();
  let nbytes = size * sizeof[real_t]();

  copy_offset(
    source.x, offset_source * sizeof[real_t](),
    dest, offset_ptr, nbytes);

  copy_offset(
    source.y, offset_source * sizeof[real_t](),
    dest, offset_ptr + nbytes, nbytes);

  copy_offset(
    source.z, offset_source * sizeof[real_t](),
    dest, offset_ptr + nbytes * 2, nbytes);

  size * 3
}

fn copy_buffer_to_struct_of_arrays(
  source: Buffer,
  offset_source: i32,
  dest: StructOfArrays3D,
  offset_dest: i32,
  size: i32) -> i32 {

  let offset_ptr = offset_source * sizeof[real_t]();
  let nbytes = size * sizeof[real_t]();

  copy_offset(
    source, offset_ptr,
    dest.x, offset_dest * sizeof[real_t](), nbytes);

  copy_offset(
    source, offset_ptr + nbytes,
    dest.y, offset_dest * sizeof[real_t](), nbytes);

  copy_offset(
    source, offset_ptr + nbytes * 2,
    dest.z, offset_dest * sizeof[real_t](), nbytes);

  size * 3
}

fn mpi_synchronize_ghost_zone(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let total_number_of_particles =
    accelerator_grid.total_number_of_clusters *
    accelerator_grid.cluster_size;

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.positions_accelerator,
    accelerator_grid.positions_cpu,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.velocities_accelerator,
    accelerator_grid.velocities_cpu,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.forces_accelerator,
    accelerator_grid.forces_cpu,
    total_number_of_particles);

  if(world_rank % 2 == 0) {
    mpi_send_ghost_zone(grid, accelerator_grid, world_size, world_rank);
    mpi_recv_ghost_zone(grid, accelerator_grid, world_size, world_rank);
  } else {
    mpi_recv_ghost_zone(grid, accelerator_grid, world_size, world_rank);
    mpi_send_ghost_zone(grid, accelerator_grid, world_size, world_rank);
  }

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.positions_cpu,
    accelerator_grid.positions_accelerator,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.velocities_cpu,
    accelerator_grid.velocities_accelerator,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.forces_cpu,
    accelerator_grid.forces_accelerator,
    total_number_of_particles);
}

fn mpi_send_exchange_cells(
  grid: &mut Grid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();

  max_send_particles = 0;

  for dest_rank, begin, end in
      mpi_send_subdomains(world_size, world_rank, *grid) {

    rank_send_particles(dest_rank) = 0;

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {

      mpih.send(
        &cell.size as MPI_MutBuf, 1, mpih.int_t,
        dest_rank, 0, mpih.comms.world);
      mpih.send(
        &cell.padding as MPI_MutBuf, 1, mpih.int_t,
        dest_rank, 2, mpih.comms.world);
      mpih.send(
        &cell.cluster_size as MPI_MutBuf, 1, mpih.int_t,
        dest_rank, 3, mpih.comms.world);

      let nparticles = cell.size + cell.padding;

      rank_send_particles(dest_rank) += nparticles;

      if(nparticles > 0) {
        /*
        print_i32(world_rank);
        print_string("> exchange_send: (");
        print_i32(index(0));
        print_string(", ");
        print_i32(index(1));
        print_string(", ");
        print_i32(index(2));
        print_string("), nparticles: ");
        print_i32(nparticles);
        print_string("\n");
        print_flush();
        */

        mpih.send(
          cell.masses.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 4, mpih.comms.world);

        mpih.send(
          cell.positions.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 5, mpih.comms.world);
        mpih.send(
          cell.positions.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 6, mpih.comms.world);
        mpih.send(
          cell.positions.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 7, mpih.comms.world);

        mpih.send(
          cell.velocities.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 8, mpih.comms.world);
        mpih.send(
          cell.velocities.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 9, mpih.comms.world);
        mpih.send(
          cell.velocities.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 10, mpih.comms.world);

        mpih.send(
          cell.forces.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 11, mpih.comms.world);
        mpih.send(
          cell.forces.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 12, mpih.comms.world);
        mpih.send(
          cell.forces.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 13, mpih.comms.world);
      }
    }

    if(max_send_particles < rank_send_particles(dest_rank)) {
      max_send_particles = rank_send_particles(dest_rank);
    }
  }
}

fn mpi_recv_exchange_cells(
  grid: &mut Grid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mut status: MPIStatus;

  max_recv_particles = 0;

  for source_rank, begin, end in
      mpi_receive_subdomains(world_size, world_rank, *grid) {

    rank_recv_particles(source_rank) = 0;

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {

      let old_nparticles = cell.size + cell.padding;

      mpih.recv(
        &cell.size as MPI_MutBuf, 1, mpih.int_t,
        source_rank, 0, mpih.comms.world, &mut status);
      mpih.recv(
        &cell.padding as MPI_MutBuf, 1, mpih.int_t,
        source_rank, 2, mpih.comms.world, &mut status);
      mpih.recv(
        &cell.cluster_size as MPI_MutBuf, 1, mpih.int_t,
        source_rank, 3, mpih.comms.world, &mut status);

      if(cell.size >= cell.capacity) {
        reallocate_cell(cell.size, cell, alloc_cpu);
      }

      let nparticles = cell.size + cell.padding;

      rank_recv_particles(source_rank) += nparticles;
      grid.nparticles += nparticles - old_nparticles;

      if(nparticles > 0) {
        /*
        print_i32(world_rank);
        print_string("> exchange_recv: (");
        print_i32(index(0));
        print_string(", ");
        print_i32(index(1));
        print_string(", ");
        print_i32(index(2));
        print_string("), nparticles: ");
        print_i32(nparticles);
        print_string("\n");
        print_flush();
        */

        mpih.recv(
          cell.masses.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 4, mpih.comms.world, &mut status);

        mpih.recv(
          cell.positions.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 5, mpih.comms.world, &mut status);
        mpih.recv(
          cell.positions.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 6, mpih.comms.world, &mut status);
        mpih.recv(
          cell.positions.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 7, mpih.comms.world, &mut status);

        mpih.recv(
          cell.velocities.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 8, mpih.comms.world, &mut status);
        mpih.recv(
          cell.velocities.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 9, mpih.comms.world, &mut status);
        mpih.recv(
          cell.velocities.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 10, mpih.comms.world, &mut status);

        mpih.recv(
          cell.forces.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 11, mpih.comms.world, &mut status);
        mpih.recv(
          cell.forces.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 12, mpih.comms.world, &mut status);
        mpih.recv(
          cell.forces.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 13, mpih.comms.world, &mut status);
      }
    }

    if(max_recv_particles < rank_recv_particles(source_rank)) {
      max_recv_particles = rank_recv_particles(source_rank);
    }
  }
}

fn mpi_exchange_ghost_zone(
  grid: &mut Grid,
  world_size: i32,
  world_rank: i32) -> () {

  if(world_rank % 2 == 0) {
    mpi_send_exchange_cells(grid, world_size, world_rank);
    mpi_recv_exchange_cells(grid, world_size, world_rank);
  } else {
    mpi_recv_exchange_cells(grid, world_size, world_rank);
    mpi_send_exchange_cells(grid, world_size, world_rank);
  }
}
