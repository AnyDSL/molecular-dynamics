fn @get_comm_time_steps() -> i32 { 2 }

fn mpi_initialize(world_size: &mut i32, world_rank: &mut i32) -> () {
  let mpih = mpi();

  mpih.init();

  mpih.comm_size(mpih.comms.world, world_size);
  mpih.comm_rank(mpih.comms.world, world_rank);
}

fn mpi_finalize() -> () {
  let mpih = mpi();

  mpih.finalize();
}

fn mpi_send_subdomains(
  world_size: i32,
  rank: i32,
  grid: Grid,
  body: fn(i32, [i32 * 3], [i32 * 3]) -> ()) -> () {

  if(rank > 0) {
    body(
      rank - 1,
      [0, 0, get_comm_time_steps()],
      [grid.nx, grid.ny, get_comm_time_steps() * 2]
    );
  }

  if(rank < world_size - 1) {
    body(
      rank + 1,
      [0, 0, grid.nz - get_comm_time_steps() * 2],
      [grid.nx, grid.ny, grid.nz - get_comm_time_steps()]
    );
  }
}

fn mpi_receive_subdomains(
  world_size: i32,
  rank: i32,
  grid: Grid,
  body: fn(i32, [i32 * 3], [i32 * 3]) -> ()) -> () {

  if(rank < world_size - 1) {
    body(
      rank + 1,
      [0, 0, grid.nz - get_comm_time_steps()],
      [grid.nx, grid.ny, grid.nz]
    );
  }

  if(rank > 0) {
    body(
      rank - 1,
      [0, 0, 0],
      [grid.nx, grid.ny, get_comm_time_steps()]
    );
  }
}

fn @mpi_get_rank_bounding_box(
  world_size: i32,
  rank: i32,
  cell_spacing: f64,
  grid_aabb: AABB) -> AABB {

  let mut zmin: f64;
  let mut zmax: f64;

  if(world_size > 1) {
    let zcells = math.floor(
      (grid_aabb.max(2) - grid_aabb.min(2)) / cell_spacing
    ) as i32;

    let zlength = (zcells / world_size) as f64 * cell_spacing;

    zmin = grid_aabb.min(2) + zlength * (rank as f64);
    zmax = grid_aabb.min(2) + zlength * ((rank + 1) as f64);

    if(rank > 0) {
      zmin -= (get_comm_time_steps() as f64) * cell_spacing;
    }

    if(rank < world_size - 1) {
      zmax += (get_comm_time_steps() as f64) * cell_spacing;
    } else {
      zmax += cell_spacing;
    }

    /*
    print_i32(rank);
    print_string("- zmin = ");
    print_f64(zmin);
    print_string(", zmax = ");
    print_f64(zmax);
    print_string(", cell_spacing = ");
    print_f64(cell_spacing);
    print_string("\n");
    */
  } else {
    zmin = grid_aabb.min(2);
    zmax = grid_aabb.max(2);
  }

  AABB {
    min: [grid_aabb.min(0), grid_aabb.min(1), zmin],
    max: [grid_aabb.max(0), grid_aabb.max(1), zmax]
  }
}

fn mpi_send_ghost_zone(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();

  for dest_rank, begin, end in
      mpi_send_subdomains(world_size, world_rank, *grid) {

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {
      let flat_index = flatten_index(index, grid);
      let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
      let offset = cell_offsets(flat_index);
      let nparticles = cell.size + cell.padding;

      if(nparticles > 0) {
        /*
        print_i32(world_rank);
        print_string("> send: (");
        print_i32(index(0));
        print_string(", ");
        print_i32(index(1));
        print_string(", ");
        print_i32(index(2));
        print_string("), nparticles: ");
        print_i32(nparticles);
        print_string(", offset: ");
        print_i32(offset);
        print_string("\n");
        print_flush();
        */

        mpih.send(
          &get_positions_x(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 1, mpih.comms.world);
        mpih.send(
          &get_positions_y(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 2, mpih.comms.world);
        mpih.send(
          &get_positions_z(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 3, mpih.comms.world);

        mpih.send(
          &get_velocities_x(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 4, mpih.comms.world);
        mpih.send(
          &get_velocities_y(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 5, mpih.comms.world);
        mpih.send(
          &get_velocities_z(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 6, mpih.comms.world);

        mpih.send(
          &get_forces_x(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 7, mpih.comms.world);
        mpih.send(
          &get_forces_y(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 8, mpih.comms.world);
        mpih.send(
          &get_forces_z(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, dest_rank, 9, mpih.comms.world);
      }
    }
  }
}

fn mpi_recv_ghost_zone(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mut status: MPIStatus;

  for source_rank, begin, end in
      mpi_receive_subdomains(world_size, world_rank, *grid) {

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {
      let flat_index = flatten_index(index, grid);
      let cell_offsets = get_array_of_i32(accelerator_grid.cell_offsets);
      let offset = cell_offsets(flat_index);
      let nparticles = cell.size + cell.padding;

      if(nparticles > 0) {
        /*
        print_i32(world_rank);
        print_string("> recv: (");
        print_i32(index(0));
        print_string(", ");
        print_i32(index(1));
        print_string(", ");
        print_i32(index(2));
        print_string("), nparticles: ");
        print_i32(nparticles);
        print_string(", offset: ");
        print_i32(offset);
        print_string("\n");
        print_flush();
        */

        mpih.recv(
          &get_positions_x(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 1, mpih.comms.world, &mut status);
        mpih.recv(
          &get_positions_y(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 2, mpih.comms.world, &mut status);
        mpih.recv(
          &get_positions_z(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 3, mpih.comms.world, &mut status);

        mpih.recv(
          &get_velocities_x(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 4, mpih.comms.world, &mut status);
        mpih.recv(
          &get_velocities_y(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 5, mpih.comms.world, &mut status);
        mpih.recv(
          &get_velocities_z(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 6, mpih.comms.world, &mut status);

        mpih.recv(
          &get_forces_x(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 7, mpih.comms.world, &mut status);
        mpih.recv(
          &get_forces_y(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 8, mpih.comms.world, &mut status);
        mpih.recv(
          &get_forces_z(accelerator_grid)(offset) as MPI_MutBuf,
          nparticles, mpih.double_t, source_rank, 9, mpih.comms.world, &mut status); 
      }
    }
  }
}

fn mpi_synchronize_ghost_zone(
  grid: &mut Grid,
  accelerator_grid: AcceleratorGrid,
  world_size: i32,
  world_rank: i32) -> () {

  let total_number_of_particles =
    accelerator_grid.total_number_of_clusters *
    accelerator_grid.cluster_size;

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.positions_accelerator,
    accelerator_grid.positions_cpu,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.velocities_accelerator,
    accelerator_grid.velocities_cpu,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.forces_accelerator,
    accelerator_grid.forces_cpu,
    total_number_of_particles);

  if(world_rank % 2 == 0) {
    mpi_send_ghost_zone(grid, accelerator_grid, world_size, world_rank);
    mpi_recv_ghost_zone(grid, accelerator_grid, world_size, world_rank);
  } else {
    mpi_recv_ghost_zone(grid, accelerator_grid, world_size, world_rank);
    mpi_send_ghost_zone(grid, accelerator_grid, world_size, world_rank);
  }

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.positions_cpu,
    accelerator_grid.positions_accelerator,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.velocities_cpu,
    accelerator_grid.velocities_accelerator,
    total_number_of_particles);

  transfer_struct_of_arrays_between_devices(
    accelerator_grid.forces_cpu,
    accelerator_grid.forces_accelerator,
    total_number_of_particles);
}

fn mpi_send_exchange_cells(
  grid: &mut Grid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();

  for dest_rank, begin, end in
      mpi_send_subdomains(world_size, world_rank, *grid) {

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {

      mpih.send(
        &cell.size as MPI_MutBuf, 1, mpih.int_t,
        dest_rank, 0, mpih.comms.world);
      mpih.send(
        &cell.padding as MPI_MutBuf, 1, mpih.int_t,
        dest_rank, 2, mpih.comms.world);
      mpih.send(
        &cell.cluster_size as MPI_MutBuf, 1, mpih.int_t,
        dest_rank, 3, mpih.comms.world);

      let nparticles = cell.size + cell.padding;

      if(nparticles > 0) {
        /*
        print_i32(world_rank);
        print_string("> exchange_send: (");
        print_i32(index(0));
        print_string(", ");
        print_i32(index(1));
        print_string(", ");
        print_i32(index(2));
        print_string("), nparticles: ");
        print_i32(nparticles);
        print_string("\n");
        print_flush();
        */

        mpih.send(
          cell.masses.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 4, mpih.comms.world);

        mpih.send(
          cell.positions.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 5, mpih.comms.world);
        mpih.send(
          cell.positions.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 6, mpih.comms.world);
        mpih.send(
          cell.positions.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 7, mpih.comms.world);

        mpih.send(
          cell.velocities.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 8, mpih.comms.world);
        mpih.send(
          cell.velocities.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 9, mpih.comms.world);
        mpih.send(
          cell.velocities.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 10, mpih.comms.world);

        mpih.send(
          cell.forces.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 11, mpih.comms.world);
        mpih.send(
          cell.forces.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 12, mpih.comms.world);
        mpih.send(
          cell.forces.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          dest_rank, 13, mpih.comms.world);
      }
    }
  }
}

fn mpi_recv_exchange_cells(
  grid: &mut Grid,
  world_size: i32,
  world_rank: i32) -> () {

  let mpih = mpi();
  let mut status: MPIStatus;

  for source_rank, begin, end in
      mpi_receive_subdomains(world_size, world_rank, *grid) {

    for cell, index in
        map_over_grid_subdomain(grid, begin, end, range, range, range) {

      let old_nparticles = cell.size + cell.padding;

      mpih.recv(
        &cell.size as MPI_MutBuf, 1, mpih.int_t,
        source_rank, 0, mpih.comms.world, &mut status);
      mpih.recv(
        &cell.padding as MPI_MutBuf, 1, mpih.int_t,
        source_rank, 2, mpih.comms.world, &mut status);
      mpih.recv(
        &cell.cluster_size as MPI_MutBuf, 1, mpih.int_t,
        source_rank, 3, mpih.comms.world, &mut status);

      if(cell.size >= cell.capacity) {
        reallocate_cell(cell.size, cell, alloc_cpu);
      }

      let nparticles = cell.size + cell.padding;

      grid.nparticles += nparticles - old_nparticles;

      if(nparticles > 0) {
        /*
        print_i32(world_rank);
        print_string("> exchange_recv: (");
        print_i32(index(0));
        print_string(", ");
        print_i32(index(1));
        print_string(", ");
        print_i32(index(2));
        print_string("), nparticles: ");
        print_i32(nparticles);
        print_string("\n");
        print_flush();
        */

        mpih.recv(
          cell.masses.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 4, mpih.comms.world, &mut status);

        mpih.recv(
          cell.positions.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 5, mpih.comms.world, &mut status);
        mpih.recv(
          cell.positions.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 6, mpih.comms.world, &mut status);
        mpih.recv(
          cell.positions.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 7, mpih.comms.world, &mut status);

        mpih.recv(
          cell.velocities.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 8, mpih.comms.world, &mut status);
        mpih.recv(
          cell.velocities.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 9, mpih.comms.world, &mut status);
        mpih.recv(
          cell.velocities.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 10, mpih.comms.world, &mut status);

        mpih.recv(
          cell.forces.x.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 11, mpih.comms.world, &mut status);
        mpih.recv(
          cell.forces.y.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 12, mpih.comms.world, &mut status);
        mpih.recv(
          cell.forces.z.data as MPI_MutBuf, nparticles, mpih.double_t,
          source_rank, 13, mpih.comms.world, &mut status);
      }
    }
  }
}

fn mpi_exchange_ghost_zone(
  grid: &mut Grid,
  world_size: i32,
  world_rank: i32) -> () {

  if(world_rank % 2 == 0) {
    mpi_send_exchange_cells(grid, world_size, world_rank);
    mpi_recv_exchange_cells(grid, world_size, world_rank);
  } else {
    mpi_recv_exchange_cells(grid, world_size, world_rank);
    mpi_send_exchange_cells(grid, world_size, world_rank);
  }
}
